---
title: "Deliverable 3"
author: "Pere Arnau Alegre & Andrés Jiménez González"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  html_document:
    toc: no
    toc_depth: '4'
geometry: left=1.0cm,right=1.0cm,top=1.25cm,bottom=1.52cm
fontsize: 10pt
subtitle: PCA, Clustering and MCA
classoption: a3paper
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
library(formatR)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

# Data description
* Description https://www.kaggle.com/datasets/adityadesai13/used-car-dataset-ford-and-mercedes
* Data Dictionary - Scraped data of used cars, which have been separated
into files corresponding to each car manufacturer (only Mercedes, BMW,
Volkswagen and Audi cars are to be considered).

## Variables
* Model
  * A string indicating the model of the car.
* Year	
  * A discrete numeric variable to indicate the year the car was sold
* Price
  * Continuous variable indicating the price at which the car was sold
* Transmission
  * Categorical variable that indicates the type of transmission of the car
  * Values:
    * Automatic
    * Manual
    * Semi-Automatic
    * Other
* Mileage
  * A discrete numeric variable to indicate the number of miles the car had when it was sold
* Fuel Type
  * Categorical variable that indicates the type of fuel of the car
  * Values:
    * Diesel
    * Electric
    * Hybrid
    * Petrol
    * Other
* Tax
  * A discrete numeric variable to indicate the road tax of the vehicle.
* MPG
  *  Continuous variable indicating the fuel consumption of the car
* Engine Size
  * Continuous variable indicating the size of the engine
* Manufacturer
  * Categorical variable that indicates the manufacturer brand of the car.
  * Values:
    * Mercedes
    * Audi
    * Volkswagen
    * BMW


# Loading of Required Packages for the deliverable
We load the necessary packages and set the working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
#setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable2")
setwd("C:/Users/Arnau/Desktop/adei/deliverable3")
# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "corrplot", "moments")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```  

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```


```{r}
#filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/adei/"
filepath <- "C:/Users/Arnau/Desktop/adei/deliverable3/"
load(paste0(filepath,"5000cars_clean.RData"))

```

# Quantitative Logistics Regression

Before we begin to see correlations with our target (variable price), we should consider the normality of this.

## Normality
```{r}
hist(df$price,100,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$price);ss<-sd(df$price)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)
shapiro.test(df$price)
```

Although the histogram of the variable price is not really symmetric, we see that the target price is normally distributed because in the shapiro test we get a p-value smaller than 0.01
so we can reject the null hypothesis.
We can see that our target variable of price is not normally distributed, as the histogram of the variable price is not symmetric and we get a p-value smaller than 0.01 in the shapiro wilk test so we can reject the null hypothesis of normality.

### Symmetry
```{r}
#moments library
skewness(df$price)
```

Normal data should have 0 skewness: we see that our data is right skewed (2.31).

### Kurtosis
```{r}
kurtosis(df$price)
```

Normal data should be 3. We have 15.1, so, in this case, our data is not normal.

## Some transformations of numeric & quantitative variables
```{r}
ll<-which(df$years_after_sell==0)
df$years_after_sell[ll]<-0.5

ll<-which(df$tax==0)
df$tax[ll]<-0.5

ll<-which(df$mileage==0)
df$mileage[ll]<-0.5

ll<-which(df$mpg==0)
df$mpg[ll]<-0.5
```


## Start to find models

Steps to follow:

1. Enter all relevant numerical variables in the model
2. See if you need to replace a number with its equivalent factor
3. Add to the best model of step 2, the main effects of the factors and retain the significant net effects.
4. Add interactions: between factor-factor and between factor-numeric (doubles).
5. Diagnosis of waste and observations. Lack of adjustment and / or influential.

A good model not only needs to fit data well, it also needs to be parsimonious. That is, a good model should be only be as complex as necessary to describe a dataset. If we are choosing between a very simple model with 1 IV, and a very complex model with, say, 10 IVs, the very complex model needs to provide a much better fit to the data in order to justify its increased complexity. If it can’t, then the more simpler model should be preferred.


### Method 1: take the most correlated numerical variables 

We use spearman's method since out target is not normally distributed

```{r}
M = round(cor(df[,c("price",vars_con)], method="spearman"),dig=2)
corrplot(M,  method = 'circle', insig='blank',
         addCoef.col ='black', number.cex = 0.8, order = 'AOE', diag=FALSE)
```

After seeing the correlation, to make an initial model, we should select the ones that are most correlated, which are:

* tax
* mileage
* years_after_sell
* mpg

### Model 1
```{r}
model_1 <- lm(price~mileage+tax+mpg+years_after_sell, data=df);summary(model_1)
```

Model_1 explains 50.76% of the variability of the target.

```{r}
vif(model_1) #Variance inflation factor: multicorrelation
```
None of the variables of our model have a vif greater than 5 so they are independent with each other.

### Model 2
With this second model we will start applying transformations to our numerical variables and response variable
```{r}
library(MASS)
# Target variable transformation?
boxcox(price~mileage+tax+mpg+years_after_sell, data=df)
# Lambda = 0 - log transformation is needed, as we predicted while testing for normality

# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df)
summary(m2)
vif(m2) #Not changed because explanatory variables have not changed


# Transformations to my regresors
residualPlots(m2,id=list(method=cooks.distance(m2),n=10))
```
From the residual plots, we see that some variables show a non linear pattern with the residuals and therefore should be applyed a transformation.

```{r}
boxTidwell(log(price)~mileage+tax+years_after_sell, data=df)
boxTidwell(log(price)~mpg+years_after_sell+mileage, data=df)
# Power transformations of the predictors in a linear model

# Suggested by BoxTidwell
m3<-lm(log(price)~I(mileage^3)+tax+I(mpg^-1)+I(years_after_sell^2),data=df)
m3_aux <- step(m3, k=log(nrow(df)) ) #to see if we can reduce the model
#step shows that no reduction is needed
summary(m3)

```

Checking if the transformations suggested by boxTidwell are actually relevant.
```{r}
anova(m2, m3)
abs(AIC(m2, m3)) #model 3 offers a better fit
abs(BIC(m2, m3)) #model 3 offers a better fit
```
The anova test shows that the 2 models are equal and from the AIC and the BIC we get that model 3 offers a better fit.

Let's try to remove multivariate outliers from the model and perform boxTidwell again.
```{r}
df_clean = df[!df$mout=="MvOut.Yes",]
m3<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
summary(m3)
```
We can see from the summary that the mileage has lost some relevancy in the model due to the removal of multivariate outliers.

```{r}
boxTidwell(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
#we see from the boxtidwell output that the mileage does not need transformation and that the tax should be square rooted
boxTidwell(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
m3_aux<-lm(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
summary(m3_aux)
m3 <- m3_aux
```
We will keep the new model3 as it offers a greater r-squared than model 2 and has a better fit with residuals and AIC score.


#### Small validation of our model before adding factors
```{r}
# Validation and effects consideration:
Anova(m3) #Net effect test
vif(m3)
par(mfrow=c(2,2))
plot(m3,id.n=0)
par(mfrow=c(1,1))
library(lmtest)
bptest(m3)
```
In the residuals vs fitted models we can observe that  the mean residual doesn't change with the fitted values (and so is doesn't change with x), and the spread of the residuals (and hence of the y's about the fitted line) is consistent as the fitted values (or x) changes. That is, the spread is constant and so we have homoscedasticity.With the Breusch-Pagan test we get a p-value of 0.2125 so with a lot of confidence we don't reject the hypothesis of homoscedasticity. The model fits the linear hypothesis.
About the test: The bp test statistic approximately follows a chi-square distribution. The null hypothesis for this test is that the error variances are all equal. More specifically, as Y increases, the variances increase (or decrease).
In the Normal QQ plot we can see that values tend to drop in value in the left tail, this effect happens because our variable log(price) is left skewed. But the effect is not too drastic so we could accept that our distribution of residuals is normal.

```{r}
residualPlots(m3,id=list(method=cooks.distance(m3),n=10))
```
We can see that all the variables from the model show linear residuals because we have applied the according transformation to all the variables.


```{r}
marginalModelPlots(m3)
```
Marginal model plots display the marginal relationship between the response and each predictor in a regression model. Marginal model plots display the dependent variable on each vertical axis and each independent variable on a horizontal axis.
There is one marginal model plot for each independent variable and one additional plot that displays the predicted values on the horizontal axis. Each plot contains a scatter plot of the two variables, a smooth fit function for the variables in the plot (labeled "Data"), and a function that displays the predicted values as a function of the horizontal axis variable (labeled "Model"). When the two functions are similar in each of the graphs, there is evidence that the model fits well. When the two functions differ in at least one of the graphs, there is evidence that the model does not fit well. We can see that our model fits well with the data.

```{r}
avPlots(m3,id=list(method=hatvalues(m3),n=5))
crPlots(m3,id=list(method=cooks.distance(m3),n=5))
library(effects)
plot(allEffects(m3))

```
Form the effect plots We see that our model defines the following:

* the log(price) decreases in a linear way as the mileage grows 
* the log(price) increases in a logarithmic way as the tax grows
* the log(price) decreases in a exponential way as the mpg grows
* the log(price) decreases in a quadratic way (-x^2) as the years_after_sell grows

## Adding factors to the model

Refactor of engineSize to include it in the model
```{r}
df_clean$engineSize_num <- as.numeric(as.character(df_clean$engineSize))
hist(df_clean$engineSize_num)
s_aux <- summary(df_clean$engineSize_num)
df_clean$engineSize <- factor(cut(df_clean$engineSize_num,breaks=c(s_aux[1],2,4,s_aux[6]),include.lowest = T ), labels=c("small_engine", "medium_engine", "large_engine"))
barplot(table(df_clean$engineSize))
```

```{r}
m4 <- update(m3, ~.+fuelType+transmission+engineSize+manufacturer,data=df_clean)
vif(m4)
#we do not have dependency among variables
summary(m4)
Anova(m4)
#we see from the Anova output that all variables greatly contribute to the regression model

par(mfrow=c(2,2))
plot(m4,id.n=0)
par(mfrow=c(1,1))
```
We can see that with this new model is fully homoscesdastic, and it is also normally distributed.

### Adding interactions 

We now have built a very strong model with high predictive power, but we could exploit the interactions between the factor variables to increase the accuracy of the model.
```{r}
m5<-update(m3, ~.+fuelType*(engineSize+transmission+manufacturer)^2, data=df_clean)
summary(m5)
#we will perform a step to remove the insignificant interactions from our model
m5_aux <- step( m5, k=log(nrow(df_clean)))
#from the step function we are left with this model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:engineSize+ fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
Anova(m5)
#we see from the Anova that the interaction between fuelType and engineSize has no relevancy, so we will remove it from our model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
```

We will now consider adding an interaction between a factor and a covariate. To choose the best pairing, we should join the couple of variables with the worst correlation in order to create a new sort of variable that will give greater information to our model. On the other hand, if we join 2 variables with strong correlation, we would be just adding redundant information because 1 variable explaing the other. From our previous analyses, we found that tax and manufacturer were not very correlated so we will pair those 2 variables.
```{r}
kruskal.test(df_clean$tax~df_clean$manufacturer)
boxplot(df_clean$tax~df_clean$manufacturer) #not so strong correlation
m6 <- update(m5, ~.+tax:manufacturer, data=df_clean)
summary(m6)
```



We will now compare model 4 and model 6 to see if the upgrade is worth it.
```{r}
#let's see if our model with improved interactions offers some difference from our previous model
anova(m6,m4) #it does

#let's compare the AIC of both models to see which has the better fit
abs(AIC(m6, m4)) #model 4 offers a better fit
abs(BIC(m6, m4)) #model 4 offers a better fit
#model 6 = 0.843
#model 4 = 0.8356
```
Model 4 also offers a greater overall fit as the AIC & BIC scores are greater, but model 6 offers a greater residual squared score, so we will stay with model 6 just for performance reasons.


## Treatment of influential data

```{r}
par(mfrow=c(2,2))
plot(m6,id.n=0)
par(mfrow=c(1,1))

```
In the residual plots we can see the same patterns as with our previous models: the model checks the linear hypothesis, presents homoscedasticity and normality. This suggests a level of remarkable consistency with the predictions of the model. We can also see some high residuals, despite having treated outlying observations during preprocessing and having eliminated multivariate outliers.

In order to the fit of our model we will treat influential observations. Influence is the combination of the leverage and outlierness of an observation.
* Leverage: An observation with an extreme value on a predictor variable is a point with high leverage. Leverage is a measure of how far an independent variable deviates from its mean. High leverage points can have a great amount of effect on the estimate of regression coefficients.
* Influence: An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. Influence can be thought of as the product of leverage and outlierness.

```{r}
library(olsrr)
hat.plot <-function(fit) {
              p <- length(coefficients(fit))
              n <- length(fitted(fit))
              plot(hatvalues(fit), main="Index Plot of Hat Values")
              abline(h=3*p/n, col="red", lty=2)
}
hat.plot(m6)
```
As a priori observation of influence we will use the hat value that is the most common measure of leverage. From the hat plot generated we can see we have a lot of observations with high leverage (the points outside the red line).

```{r}
ols_plot_cooksd_chart(m6)
```
As a posteriori observation of influence we will use the cook's distance because it is the most common measure of influence. From the cook's D chart generated we can see we have a lot of observations with high influential level.

Finally to choose the influential observations we will use Cook's distance because it offers a consistent method for detecting them. We could use a mixture of hat values (leverage obs.) and rstudent residuals (outlying obs.) to predict influential observations, but it is not as efficient and reliable.
```{r}
# Define initial parameters to calculate obs. with high leverage:
p <-length(m6$coefficients)
n <- length(m6$fitted.values)
hat_param <- 3
#we use as a threshold to lavel the leverage of the obs 3*p/n because we have a large dataset

# A priori influential observation
influencePlot(m6, id=list(n=10))
ll_priori_influential <- which(abs(hatvalues(m6))>hat_param*(p/n))
length(ll_priori_influential)
ll_rst <- which(abs(rstudent(m6))>3);
length(ll_rst)
ll_hat_rst <- unique(c(ll_priori_influential,ll_rst))
length(ll_hat_rst)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m6))>(4/(n-p)))
length(ll_posteriori_influential)


m7_hat_rst<-update(m6, data=df_clean[-ll_hat_rst,])
summary(m7_hat_rst)

m7_cook <- update(m6, data=df_clean[-ll_posteriori_influential,])
summary(m7_cook)

m7 <- m7_cook
```
As expected, the model with the removal of influential data using cook's distance has a better fit.


## Diagnostics of our final Model
```{r}
Anova(m7)
#all variables and interactions are relevant
par(mfrow=c(1,1))
plot(allEffects(m7), selection=1)
plot(allEffects(m7), selection=2)
plot(allEffects(m7), selection=3)
plot(allEffects(m7), selection=4)
plot(allEffects(m7), selection=5)
plot(allEffects(m7), selection=6)
plot(allEffects(m7), selection=7)
```
We see that our model defines the following:

* the log(price) decreases in a linear way as the mileage grows
* the log(price) increases as tax increases until tax=140, then log(price) decreases as tax increases
* the log(price) decreases in an exponential way as the mpg grows
* the log(price) decreases in a quadratic way (-x^2) as the years_after_sell grows
* the log(price) is higher with BMW, Merc and Audi cars than VW cars
* the log(price) is higher with diesel and hybrid cars than petrol cars
* the log(price) is higher with automatic and semi-automatic cars than manual cars
* the log(price) is higher with larger engines


```{r}
par(mfrow=c(2,2))
plot(m7,id.n=0)
par(mfrow=c(1,1))
```
In the residual plots we can see the same patter as before, the model presents homoscedasticity and normality and fits the linear hypothesis. This suggests a level of remarkable consistency with the predictions of the model. We can also see the effect of the deletion of infuential data as there are not outlying residuals.

```{r}
marginalModelPlots(m7)
```
We can see from our plots that the model fits perfectly with the data.

```{r}
#avPlots(m7)
#We will abstain from plotting the added variable plots because they do not give us additional information and we would also need to analyze 45 graphs
par(mfrow=c(1,1))
influencePlot(m7, id=list(n=10))
```
From a look at the influence plot, we can see a clar difference from the influence plot of model6. We can see now that we do not have any influential observation. We only have some obs. with some leverage but without being outliers and vice versa.

```{r}
df_clean2=df_clean[-ll_posteriori_influential,]
res.pca <- PCA(df_clean2[c(5,7,8,13)], graph=FALSE )
plot(res.pca$ind$coord[,1], log(df_clean2$price), pch=19,col="darkblue", main="PCA of variables vs Actual/Predicted price")
points(res.pca$ind$coord[,1], m7$fitted.values, col="red", pch=20)
legend("topright", legend=c("Actual", "Predicted"),
       col=c("darkblue", "red"), pch=19, cex=0.8,
       title="Point types")

#plot(x=c(1:length(m7$fitted.values)), log(df_clean2$price), col="darkgreen", pch=20)
#points(x=c(1:length(m7$fitted.values)), y=m7$fitted.values, col="red", pch=19)

```
As a final experiment to see how the model was behaving, we performed a pca with just numeric variables and we plotted the first dimension of the pca in the x-axis and the actual price and predicted price on the y-axis. We can see from our graph that our model does very good predictions as it almost fits all the data. With a perfect model, we would have a perfect overlapping of the 2 points.




