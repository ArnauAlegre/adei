---
title: "Deliverable 1"
author: "Pere Arnau Alegre & Andrés Jiménez González"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  html_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: Data Processing, Description, Validation and Profiling
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
library(formatR)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

# Data description
* Description https://www.kaggle.com/datasets/adityadesai13/used-car-dataset-ford-and-mercedes
* Data Dictionary - Scraped data of used cars, which have been separated
into files corresponding to each car manufacturer (only Mercedes, BMW,
Volkswagen and Audi cars are to be considered).

## Variables
* Model
  * A string indicating the model of the car.
* Year	
  * A discrete numeric variable to indicate the year the car was sold
* Price
  * Continuous variable indicating the price at which the car was sold
* Transmission
  * Categorical variable that indicates the type of transmission of the car
  * Values:
    * Automatic
    * Manual
    * Semi-Automatic
    * Other
* Mileage
  * A discrete numeric variable to indicate the number of miles the car had when it was sold
* Fuel Type
  * Categorical variable that indicates the type of fuel of the car
  * Values:
    * Diesel
    * Electric
    * Hybrid
    * Petrol
    * Other
* Tax
  * A discrete numeric variable to indicate the road tax of the vehicle.
* MPG
  *  Continuous variable indicating the fuel consumption of the car
* Engine Size
  * Continuous variable indicating the size of the engine
* Manufacturer
  * Categorical variable that indicates the manufacturer brand of the car.
  * Values:
    * Mercedes
    * Audi
    * Volkswagen
    * BMW


# Loading of Required Packages for the deliverable
We load the necessary packages and set the working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
#setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/deliverable2")
setwd("C:/Users/Arnau/Desktop/adei/deliverable2")
# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "corrplot")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```  

## Select a sample of 5000 records
From the proposed database, we need to select a sample of 5000 records randomly so we can start analyzing our data.
```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```


```{r}
#filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/"
filepath <- "C:/Users/Arnau/Desktop/adei/"
df<-read.table(paste0(filepath,"/sample_5000.csv"),header=T, sep=",")[c(-1)]

# dim(df)       # Displays the sample size
# names(df)     # Displays the names of the sample variables
# summary(df)   
```


## Some useful functions
```{r}
calcQ <- function(x) { # Function to calculate the different quartiles
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) 
}
countNA <- function(x) { # Function to count the NA values
  mis_x <- NULL
  for (j in 1:ncol(x)) {mis_x[j] <- sum(is.na(x[,j])) }
  mis_x <- as.data.frame(mis_x)
  rownames(mis_x) <- names(x)
  mis_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {mis_i <- mis_i + as.numeric(is.na(x[,j])) }
  list(mis_col=mis_x,mis_ind=mis_i) 
}
countX <- function(x,X) { # Function to count a specific number of appearences
  n_x <- NULL
  for (j in 1:ncol(x)) {n_x[j] <- sum(x[,j]==X) }
  n_x <- as.data.frame(n_x)
  rownames(n_x) <- names(x)
  nx_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {nx_i <- nx_i + as.numeric(x[,j]==X) }
  list(nx_col=n_x,nx_ind=nx_i) 
}
```

# Univariate Description and Preprocessing

## Variable initialization of missings, outliers and errors for columns & rows
```{r}
jmis<-rep(0,2*ncol(df))  # columns - variables
imis<-rep(0,nrow(df))  # rows - cars

mis1<-countNA(df)
#mis1$mis_ind   # Number of missings for the current set of cars (observations)
#mis1$mis_col   # Number of missings for the current set of variables

jouts<-rep(0,ncol(df))  # columns - variables
iouts<-rep(0,nrow(df))  # rows - cars

jerrs<-rep(0,ncol(df))  # columns - variables
ierrs<-rep(0,nrow(df))  # rows - cars

df$model<-factor(paste0(df$manufacturer,"-",df$model))

var_out<-calcQ(df$year)

llout<-which((df$year <= var_out$souti))
iouts[llout] <- iouts[llout]+1
jouts[which(colnames(df)=="year")]<-length(llout)

df[llout,"year"] <- NA


df$f.price<-factor(cut(df$price/1000,breaks=c(quantile(df$price/1000,seq(0,1,0.2),na.rm=TRUE)), labels=c("super cheap", "cheap", "expensive", "very expensive", "extremly expensive"),include.lowest = T ))

sel<-which(df$price <= 0)
jerrs[which(colnames(df)=="price")] <- length(sel)

df[which(df$price < 0), ] <- NA

var_out<-calcQ(df$price)

llout_price<-which((df$price > var_out$souts) | (df$price < var_out$souti ))
jouts[which(colnames(df)=="price")]<-length(llout_price)
iouts[llout_price] <- iouts[llout_price]+1

df$transmission <- factor( df$transmission )
df$transmission <- factor( df$transmission, levels = c("Manual","Semi-Auto","Automatic"),labels = paste0("f.Trans-",c("Manual","SemiAuto","Automatic")))

var_out<-calcQ(df$mileage)
llout_mil<-which((df$mileage<var_out$souti)|(df$mileage>var_out$souts))
iouts[llout_mil]<-iouts[llout_mil]+1
df[llout_mil,"mileage"] <- NA

df$fuelType <- factor(df$fuelType)
df$fuelType <- factor( df$fuelType, levels = c("Diesel","Petrol","Hybrid"), labels = paste0("f.Fuel-",c("Diesel","Petrol","Hybrid")))

var_out<-calcQ(df$tax)

llout<-which((df$tax <= var_out$souti & df$tax >= var_out$souts))
iouts[llout] <- iouts[llout]+1
jouts[which(colnames(df)=="tax")]<-length(llout)
df[llout, "tax"] <- NA

var_out<-calcQ(df$mpg)
llout_mpg<-which((df$mpg<var_out$souti)|(df$mpg>var_out$souts))
iouts[llout_mpg]<-iouts[llout_mpg]+1
jouts[which(colnames(df)=="mpg")]<-length(llout)
df[llout_mpg,"mpg"] <- NA

df$engineSize <- factor(df$engineSize)

sel<-which(df$engineSize==0)
ierrs[sel]<-ierrs[sel]+1 #Vector of errors per individual update

df[sel,"engineSize"]<-NA

library(missMDA)
vars_con<-c("year", "mileage", "tax", "mpg")
vars_res<-c("price", "Audi")

res.impca<-imputePCA(df[,vars_con],ncp=3)

df[,vars_con ]<-res.impca$completeObs
df$year<-round(df$year, digits=0)
df$mpg<-round(df$mpg, digits=2)

vars_dis<-c("model","transmission","fuelType","engineSize","manufacturer")
res.immca<-imputeMCA(df[,vars_dis],ncp=4)
df[ , vars_dis ]<-res.immca$completeObs


df$Audi<-ifelse(df$manufacturer == "Audi",1,0)
df$Audi<-factor(df$Audi,labels=c("No","Yes"))

df$years_after_sell <-  2022 - df$year

quants <- calcQ(df$tax)

df$f.tax<-factor(cut(df$tax, breaks=c(quants$min,quants$q1, quants$q2, quants$q3+10, quants$max), include.lowest=T))

df$f.mileage<-factor(cut(df$mileage,breaks=c(quantile(df$mileage,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))

df$f.mpg<-factor(cut(df$mpg,breaks=c(quantile(df$mpg,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))

df$f.year<-factor(cut(df$year,breaks=c(quantile(df$year,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))

df$inconsistencies <- imis+iouts+ierrs

res.mout <- Moutlier( df[ ,c(2,3,5,8)], quantile = 0.995) 
par(mfrow = c(1,1))
plot( res.mout$md, res.mout$rd )
abline( h=res.mout$cutoff, lwd=2, col="red") 
abline( v=res.mout$cutoff, lwd=2, col="red")
llmout <- which( ( res.mout$md > res.mout$cutoff ) & (res.mout$rd > res.mout$cutoff) )
llmout
df$mout <- 0 
df$mout[ llmout ] <- 1 
df$mout <- factor( df$mout, labels = c("MvOut.No","MvOut.Yes"))
res.mout$cutoff
```

# Principal Component Analysis
## Calculate total variance explained by each principal component
```{r}
vars_con <- c("mileage", "tax", "mpg", "years_after_sell", "inconsistencies")
vars_dis <- c("transmission", "fuelType", "engineSize", "manufacturer")
vars_res <- c("price", "Audi")
res.pca<-PCA(df[,vars_con], ind.sup = llmout)
summary(res.pca)
```

According to the Kaiser criteria we should keep 2 dimensions, because it says that we should keep that dimensions with variance > 1.

## Elbow
```{r}
fviz_screeplot(
  res.pca, 
  addlabels=TRUE, 
  ylim=c(0,50), 
  barfill="darkslateblue", 
  barcolor="darkslateblue",
  linecolor = "skyblue1"
)

```

As we have a not-so-ideal scree plot curve, we have to choose between a couple ways of deciding how many dimensions keep:
Kaiser rule: pick PCs with eigenvalues of at least 1.
Proportion of variance plot: the selected PCs should be able to describe at least 80% of the variance.

If we follow Kaiser rule we should keep only 2 dimensions, but considering that the 3rd dimensions variance is 0.965 (which is very close to 1) and that with only 2 dimensions we don't describe at least 80% of the variance, we have decided to keep 3 dimensions.

## Interpreting the axes
Variables point of view coordinates, quality of representation, contribution of the variables
### Axe 1
```{r}
res.dimdes <- dimdesc( res.pca, axes = 1:3, proba = 0.01)
res.dimdes$Dim.1
```

Dimension 1 has a great direct correlation with variables years_after_sell, mileage and mpg;
and a great inverse correlation with variable tax.

### Axe 2
```{r}
res.dimdes$Dim.2
```

Dimension 2 has a great direct correlation with variable inconsistencies;
and a low inverse correlation with variables tax, years_after_sell and mileage

### Axe 3
```{r}
res.dimdes$Dim.3
```

Dimension 3 has a good direct correlation with variable inconsistencies and tax;
and a good inverse correlation with variables mpg.

```{r}
res.pca<-PCA(df[,vars_con], ind.sup = llmout)
```

## Detection of multivariant outliers and influent data
```{r}
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.40)
```

We can see that there are some individuals that are too contributive. So now, let s try to understand them better with extreme individuals.

### Extreme individuals
#### In dimension 1:
```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:19]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:19]
```

#### In dimension 2:
```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:28]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:28]
```

## PCA taking into account also supplementary variables
Our supplementary variables are Price and Audi.
```{r}
res.pca<-PCA(df[,c(vars_res,vars_con, vars_dis)], ind.sup = llmout, quanti.sup = 1, quali.sup = c(2,8:11))
```

The variable price is strongly negatively related with the first dimension axis. This means that it has a great correlation with variable tax and inverse correlation with variables mpg, mileage and years_after_sell. 

The variable Audi has very similar centroids


# Kmeans Classification
We determined that we had to keep 3 dimensions of the PCA to maintain an inertia above 80%. Therefore we will have to use 3 dimensions for the computation of the Kmeans.

## Optimal number of clusters
```{r}
fviz_nbclust(res.pca$ind$coord[,1:3], kmeans, method = "wss")
```
Using the elbow method, we determined that the optimal number of clusters to compute for kmeans was 5. With 5 clusters we retain sufficient inertia and also we keep the within distance small and the between distance big.

## Kmeans computation and visualization
```{r}
#dis<-dist(res.pca$ind$coord[,1:2])
set.seed(1)
#when setting a seed before the generation of kmeans we force kmeans to generate the same result (a cluster with an inertia > 0.75)
res.km<-kmeans(res.pca$ind$coord[,1:3],5)
#res_aux<-table(res.km$cluster)
#while(res.km$betweenss/res.km$totss < 0.75 & res_aux[1]!=1979 & res_aux[2]!=547 &
 #   res_aux[3]!=1159 & res_aux[4]!=1069){
#  res.km<-kmeans(res.pca$ind$coord[,1:3],5)
#  res_aux<-table(res.km$cluster)
#}

res.km$betweenss/res.km$totss   #calculate total retained inertia
table(res.km$cluster)
ff<-factor(res.km$cluster)
plot(res.pca$ind$coord[,1:3],col=ff, pch=19, main= "K-Means - 5 cluster - First Factorial Plane")
legend("bottomleft", title="K-Means", legend=levels(ff), col=1:5, pch=19, cex=0.8)
```

### Gain in inertia (in %)
After executing kmeans we get a retained inertia of the 78.88098%
```{r}
100*(res.km$betweenss/res.km$totss)
```

## Profiling of clusters
```{r}
df$kmeans_clust <-0
k=5
df[-llmout,"kmeans_clust"]<-res.km$cluster
df[llmout,"kmeans_clust"]<-k+1
df$kmeans_clust <- factor(df$kmeans_clust)
# observations that are multivariant outliers will be put in cluster 6
barplot(table(df$kmeans_clust),col="darkslateblue",border="darkslateblue",main="[k-means]#observations/cluster")

res.cat <- catdes(df[c(2:12, 18:20)], num.var=14, proba=0.01)
```

We proceed to explain the data obtained.

### Description of clusters by categorical variables

```{r}
res.cat$test.chi2
```

We start with the description of the categorical variables that characterize the clusters, so in this output we do
not have dimensions because it is the total association. We can see the intensity of the variables, in our case the variables that affect more to the clustering are **fuelType**, **engineSyze** and **f.price**, because are the one with the smallest p.value.
We excluded from this test the factor variables that resulted from the grouping of the quantiles of the numerical variables because we will analyze these numerical variables later on. This way we reduce redundant information.

Next, we want to see for each cluster which are the categories that characterize them.

```{r}
res.cat$category
```

* Cluster 1
  + The first thing we can notice is that cluster 1 contains 28.7% of all cars with engineSize=3 in the sample. On average 11.42% of the cars have an engine of that size, but in cluster 1 those cars are overrepresented (29.98%). In addition, cheap cars are overrepresented (20% of global mean vs 32.5% mean in cluster 1). Furthermore, automatic cars are overrepresented (26.4% of global mean vs 39.3% mean in cluster 1). Finally, the cluster has very few expensive and extremly expensive cars.
  
* Cluster 2
  + The first thing we can notice is that cluster 1 contains 71.21% of hybrid fuel cars in the sample. On average 1.32% of the cars use hybrid fuel, but in cluster 1 hybrid cars are overrepresented (57.32%). Diesel cars represent 57.2% of the sample, but 0.87% of them are included in Cluster 1. The same can be seen for cars that run on petrol. Cars with an engineSize of 1 represent 7.48% of the sample, but 0% of them are included in Cluster 1. Finally, on average 26.4% of the cars use an automatic transmission, but in cluster 1 automatics cars are overrepresented (57.32%) and manual and semi-automatic cars are underrepresented.
  
* Cluster 3
  + The first thing we can notice is that cluster 3 contains 82.6% of all extremly expensive cars in the sample. On average 19.9% of the cars are extremly expensive, but in cluster 3 those cars are overrepresented (41.54%). In addition, very expensive cars are overrepresented (20% of global mean vs 34.5% mean in cluster 3). Furthermore, semi-automatic cars are overrepresented (38.78% of global mean vs 49.62% mean in cluster 1). Finally, manual, cheap and super cheap cars are underrepresented in the class.
  
* Cluster 4
  + In cluster 4 we can see that there is more expensive cars than in the gobal (38.3951682% in cluster 4 vs. 19.98% in global). It happens the same with cheap cars (25.6255393% in cluster 4 vs. 20.04% in global). But for the super cheap and extremely expensive cars is the opposite, they are underrepresented. For the extremely expensive cars the global percentage is 19.90%, while the percentage in cluster 4 is 5.4357204%, and for super cheap cars the global percentage is 20.00% and the cluster 4 percentage is 10.8714409%. So we can say that cluster 4 has the cars with an average price similar to the mean.
  
* Cluster 5
  + Cluster 5 is defined by the cheapest cars. We can confirm this it contains a great percentage of super cheap and cheap cars and a very little percentage of extremely expensive and very expensive cars. The percentage of super cheap cars in cluster 5 is 53.51% while in the global is 20.00%, and the percentage of cheap cars in cluster 5 is 34.89% while in the global is 20.04%. We can observe the opposite behaviour with the expensive cars: the percentage of extremely expensive cars in cluster 5 is 1.12% while in the global is 20.08%, and the percentage of cheap cars in cluster 5 is 0% while in the global is 19.90%. We can also highlight that it has a lot of cars with manual transmission (61.83% in cluster 4 vs. 34.82% in global).

* Cluster 6
 + This cluster groups all multivariant outliers in the sample. What we can observe from this cluser is that contains extreme observations like super cheap and extremely expensive cars and cars with a very high engine size. Also, cars with automatic transmission and that are BMW are overrepresented while semi-auto, expensive and very expensive cars are underrepresented.


We now proceed to see the quantitative variables that characterizes the clusters. We can see in the output from res.cat$quanti.var all the numeric variables that characterize the clusters. From a more detailed look, variables **year**, **mileage**, **tax** and **inconsistencies** maintain a strong relation with the cluster number because of their high eta2 value.
```{r}
res.cat$quanti.var
res.cat$quanti
```


We want to know now which variables are associated with the quantitative variables.

* Cluster 1
  + We can see that for cluster 1 we have cars that have a lot of mileage (average mean of class of 47044 vs 23289 overall mean) and cars with high taxes (60 units over the overall mean). The cluster contains cheaper cars than the global mean (4230 units cheaper on average). It also contains cars with lower mpg and cars that were sold on before the average selling year of the dataset. Finally the cluster does not cocntain any obervation with an inconsistency.
  
* Cluster 2
  + For cluster 2, we have cars with a higher mileage than the global average (30203 mean in cluster 2 vs 23289 global mean). We also have cars that have lower taxes. This cluster groups almost all observations with inconsistencies. Some other observations with inconsistencies are in cluster 6.
  
* Cluster 3
  +  This cluster gropus cars that were sold in 2019 on average. The cars also have a higher average price (6603 units over the global mean). In addition, these cars have lower taxes. Finally, we do not have any observation with an inconsistency.
  
* Cluster 4
  + In cluster 4 we can't see a great difference between global mean and the mean of the cluster but we can highlight that cars in cluster 4 have more mpg and taxes than in the global mean. The global mean of tax is 122.91 and its mean in cluster 5 is 141.33, and the global mean of mpg is 53 and its mean in cluster 5 is 57.46.
  
* Cluster 5
  + In cluster 5 we can highlight that the price of the cars is very cheap, the mean of the cluster is 12808.48 while the global mean is almost double: 21418.53. It is also very remarkable that the taxes are very low, the mean of the cluster is 22.61 while the global mean is almost double: 122.91. This variables have lower mean that the global but we also have variables that highlight for having a higher mean, like mpg and mileage. The global mean of mpg is 53 and its mean in cluster 5 is 65.27, and the global mean of mileage is 23289.52 and its mean in cluster 5 is 38085.44.
  
* Cluster 6
+ In cluster 6 we can see that the inconsistencies are higher that in the overall mean, as well as the mileage, tax and price:
                 Mean in category Overall mean sd in category
inconsistencies  0.3597561        0.02960
mileage          59000.55         23289.51910
tax              172.3171         122.91100
price            29204.11         21418.53580

The variable mean mpg in cluster 6 (44.96) is lower than the overall mean (53), and the cars in cluster 6 are older because the mean of the variable year is 2014 and in the overall mean is 2017.

## HCPC computation and visualization
```{r}
#dis<-dist(res.pca$ind$coord[,1:2])
res.hcpc <- HCPC(res.pca,nb.clust = -1, proba = 0.01)
100*(res.hcpc$call$t$within[1]-res.hcpc$call$t$within[1:10])/(res.hcpc$call$t$within[1])   #calculate total retained inertia
# Individuals facor map
fviz_cluster(res.hcpc, geom = "point", main = "Factor map")
```

### Gain in inertia (in %)
After executing HCPC we get a retained inertia of the 63.88594%
```{r}
100*(res.hcpc$call$t$within[1]-res.hcpc$call$t$within[4])/(res.hcpc$call$t$within[1])
```

## Profiling of clusters
```{r}
df$HCPC_clust <-0
k=4
df[-llmout,"HCPC_clust"]<-res.hcpc$data.clust$clust
df[llmout,"HCPC_clust"]<-k+1
df$HCPC_clust <- factor(df$HCPC_clust)
# observations that are multivariant outliers will be put in cluster 6
barplot(table(df$HCPC_clust),col="darkslateblue",border="darkslateblue",main="[HCPC]#observations/cluster")
```

We proceed to explain the data obtained.

### Description of clusters by categorical variables

```{r}
res.hcpc$desc.var$test.chi2
```

We can see the intensity of the variables, in our case the variables that affect more to the clustering are **fuelType**, **engineSize**,**transmission** and **manufacturer**  because are those ones with the smallest p.value.
We excluded from this test the factor variables that resulted from the grouping of the quantiles of the numerical variables because we will analyze these numerical variables later on. This way we reduce redundant information.

Next, we want to see for each cluster which are the categories that characterize them.

```{r}
res.hcpc$desc.var$category
```

* Cluster 1
  + The first thing we can notice is that cluster 1 contains the 60.052632% of all the cars with semi-automatic transmission. The 49.84894260% of its cars use petrol while the global percenatge of cars that use petrol is 41.4598842%, that means that is overrepresented. 
  
* Cluster 2
  + In cluster 2 we can see that The 65.28497409% of its cars use diesel while the global percenatge of cars that use petrol is 57.2373863%, that means that is overrepresented. The manufacturer BMW as well as cars with automatic transmission are overrepresented.
  
* Cluster 3
  + Cluster 3 contains 74.6% of all hybrid cars from the data set. On average 1.3% of the cars use hybrid fuel, but in cluster 3 hybrid cars are overrepresented (57.32%). Automatic cars are also overrepresented in the class with a 57.3% in comparison to a 25.8% global mean. BMW cars are also overrepresented in cluster 3 with a 41.46% in comparison to a 21.2% global mean.

* Cluster 4
  + Cluster 4 contains 40.2% of all manual cars from the data set. On average 34.8% of the cars are manual, but in cluster 4 manual cars are overrepresented (62.3%). Automatic cars are also overrepresented in the class with a 57.3% in comparison to a 25.8% global mean. VW cars are overrepresented where on the other hand, Mercedes and BMW cars are underrepresented.

We now proceed to see the quantitative variables that characterizes the clusters. We can see in the output from **res.hcpc$desc.var$quanti.var** all the numeric variables that characterize the clusters. From a more detailed look, variables **years_after_sell**, **mileage**, **tax** and **inconsistencies** maintain a strong relation with the cluster number.
```{r}
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

We want to know now which variables are associated with the quantitative variables.

* Cluster 1
  + Cluster 1 groups cars with higher prices than the overall mean. The mean price in this first cluster is 27024 while the overall mean sits at 21154. It also contains cars that pay higher taxes and cars that have higher fuel consumption (low mpg). Finally, the cluster contains cars with a very low mileage (7200 on avg) compared to the global mean of 22000 and the cars where sold recently, as they average year after sell is 3 for the cluster and 4.7 for the global mean.

* Cluster 2
  +Cluster 2 contains cars that where sold earlier than other cars, as they average year after sell is 6 for the cluster and 4.7 for the global mean. It also contains cars with a higher mileage and higher tax. Finally it contains cheaper cars than the global mean, 17850 dollars for the cluster vs 21100 for the overall mean.
  
* Cluster 3
  + In this cluster we can see that the variable mileage is higher than the overall mean while the taxes are lower. This can indicate us that they are old or very used cars. The most important trait for this cluster is that it groups observations that only have inconsistencies. The price is not highlighted so it must be arround the global mean. 
  
* Cluster 4
  + Cars in cluster 4 have the biggest mileage mean, being it 37241.21087. Its cars spend also less fuel than the overall mean. It is very important to highlight that the cars in cluster 4 are very cheap and have very low taxes.

# CA analysis
CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors.

## CA: f.price vs f.tax

We start the CA with the creation of a contingency table and a Pearson's chi2 test to test for independence.
```{r}
tt<-table(df[,c("f.price","f.tax")])
tt
chisq.test(tt) #to see if variables are independents. H0: Variables are independent
```
We get a p-value lower than 0.05 so we can reject the H0. In our sample, the row and the column variables are statistically significantly associated.

We are now going to take a look to the simple correspondences. 
```{r}
res.ca <- CA(tt)
summary(res.ca,dig=2)
fviz_eig(res.ca) #Visualize the eigenvalues
```
According to the Kaiser criteria, we have to consider the first dimension. In this correpsondence analysis almost all the variance is explained in the 1st dimension (96.74%).

```{r}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
legend("bottomleft", title="Lines", legend=c("Tax", "Price"), col=c("red","blue"), pch=19, cex=0.8)
```
This graph shows that with mildly high taxes comes cars with prices that are average or high. With cheap cars comes very low taxes. Finally very high taxes come with cars that are priced between 17400 and 19400 dollars. These cars would fall into the avg/cheap priced ones.


## CA: f.price vs fuelType

We start the CA with the creation of a contingency table and a Pearson's chi2 test to test for independence.
```{r}
tt<-table(df[,c("f.price","fuelType")])
tt
chisq.test(tt) #to see if variables are independents. H0: Variables are independent
```
We get a p-value lower than 0.05 so we can reject the H0. In our sample, the row and the column variables are statistically significantly associated.

We are now going to take a look to the simple correspondences. 
```{r}
res.ca <- CA(tt)
fviz_eig(res.ca) #Same outputs as PCA
summary(res.ca,dig=2)
```
According to the Kaiser criteria, we have to consider the first dimension. In this correspondence analysis almost all the variance is explained in the 1st dimension (97.1%).

```{r}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
legend("bottomleft", title="Lines", legend=c("FuelType", "Price"), col=c("red","blue"), pch=19, cex=0.8)
```

### Contribution of rows to the dimensions
```{r}
res.ca$row$contrib
fviz_contrib(res.ca, choice = "row", axes = 1:2, top = 10)
```
The row variables with the larger value, contribute the most to the definition of the dimensions.
Rows that contribute the most to Dim.1 are the most important in explaining the variability in the data set. In our case, the most important row categories for price are [1.25,10.3] & (15.3,17.4].
Rows that do not contribute much to any dimension or that contribute to the last dimensions (Dim 2) are less important. In our case these are (13,15.3] & (19.4,21.5] 

The most important (or, contributing) row points can be highlighted on the scatter plot as follow:
```{r}
fviz_ca_row(res.ca, col.row = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)
```
The scatter plot gives an idea of what pole of the dimensions the row categories are actually contributing to. It is evident that the row category (15.3,17.4] has an important contribution to the positive pole of the first dimension, while the category [1.25, 10.3] has a major contribution to the negative pole of the first dimension.

In other words, dimension 1 is mainly defined by the opposition of(15.3,17.4] (positive pole), and [1.25, 10.3] (negative pole).






