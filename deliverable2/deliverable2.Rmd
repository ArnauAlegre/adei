---
title: "Deliverable 2"
author: "Pere Arnau Alegre & Andrés Jiménez González"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
  html_document:
    toc: no
    toc_depth: '4'
geometry: left=1.0cm,right=1.0cm,top=1.25cm,bottom=1.52cm
fontsize: 10pt
subtitle: PCA, Clustering and MCA
classoption: a3paper
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
library(formatR)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

# Data description
* Description https://www.kaggle.com/datasets/adityadesai13/used-car-dataset-ford-and-mercedes
* Data Dictionary - Scraped data of used cars, which have been separated
into files corresponding to each car manufacturer (only Mercedes, BMW,
Volkswagen and Audi cars are to be considered).

## Variables
* Model
  * A string indicating the model of the car.
* Year	
  * A discrete numeric variable to indicate the year the car was sold
* Price
  * Continuous variable indicating the price at which the car was sold
* Transmission
  * Categorical variable that indicates the type of transmission of the car
  * Values:
    * Automatic
    * Manual
    * Semi-Automatic
    * Other
* Mileage
  * A discrete numeric variable to indicate the number of miles the car had when it was sold
* Fuel Type
  * Categorical variable that indicates the type of fuel of the car
  * Values:
    * Diesel
    * Electric
    * Hybrid
    * Petrol
    * Other
* Tax
  * A discrete numeric variable to indicate the road tax of the vehicle.
* MPG
  *  Continuous variable indicating the fuel consumption of the car
* Engine Size
  * Continuous variable indicating the size of the engine
* Manufacturer
  * Categorical variable that indicates the manufacturer brand of the car.
  * Values:
    * Mercedes
    * Audi
    * Volkswagen
    * BMW


# Loading of Required Packages for the deliverable
We load the necessary packages and set the working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
#setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable2")
setwd("C:/Users/Arnau/Desktop/adei/deliverable2")
# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "corrplot")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```  

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```


```{r}
#filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/adei/"
filepath <- "C:/Users/Arnau/Desktop/adei/"
df<-read.table(paste0(filepath,"/sample_5000.csv"),header=T, sep=",")[c(-1)]

# dim(df)       # Displays the sample size
# names(df)     # Displays the names of the sample variables
# summary(df)   
```


## Some useful functions
```{r}
calcQ <- function(x) { # Function to calculate the different quartiles
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) 
}
countNA <- function(x) { # Function to count the NA values
  mis_x <- NULL
  for (j in 1:ncol(x)) {mis_x[j] <- sum(is.na(x[,j])) }
  mis_x <- as.data.frame(mis_x)
  rownames(mis_x) <- names(x)
  mis_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {mis_i <- mis_i + as.numeric(is.na(x[,j])) }
  list(mis_col=mis_x,mis_ind=mis_i) 
}
countX <- function(x,X) { # Function to count a specific number of appearences
  n_x <- NULL
  for (j in 1:ncol(x)) {n_x[j] <- sum(x[,j]==X) }
  n_x <- as.data.frame(n_x)
  rownames(n_x) <- names(x)
  nx_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {nx_i <- nx_i + as.numeric(x[,j]==X) }
  list(nx_col=n_x,nx_ind=nx_i) 
}
```


```{r, fig.show='hide', include=FALSE}
#Univariate Description and Preprocessing
##Variable initialization of missings, outliers and errors for columns & rows

jmis<-rep(0,2*ncol(df))  # columns - variables
imis<-rep(0,nrow(df))  # rows - cars

mis1<-countNA(df)
#mis1$mis_ind   # Number of missings for the current set of cars (observations)
#mis1$mis_col   # Number of missings for the current set of variables

jouts<-rep(0,ncol(df))  # columns - variables
iouts<-rep(0,nrow(df))  # rows - cars

jerrs<-rep(0,ncol(df))  # columns - variables
ierrs<-rep(0,nrow(df))  # rows - cars

df$model<-factor(paste0(df$manufacturer,"-",df$model))

var_out<-calcQ(df$year)

llout<-which((df$year <= var_out$souti))
iouts[llout] <- iouts[llout]+1
jouts[which(colnames(df)=="year")]<-length(llout)

df[llout,"year"] <- NA


df$f.price<-factor(cut(df$price/1000,breaks=c(quantile(df$price/1000,seq(0,1,0.2),na.rm=TRUE)), labels=c("super cheap", "cheap", "expensive", "very expensive", "extremely expensive"),include.lowest = T ))

sel<-which(df$price <= 0)
jerrs[which(colnames(df)=="price")] <- length(sel)

df[which(df$price < 0), ] <- NA

var_out<-calcQ(df$price)

llout_price<-which((df$price > var_out$souts) | (df$price < var_out$souti ))
jouts[which(colnames(df)=="price")]<-length(llout_price)
iouts[llout_price] <- iouts[llout_price]+1

df$transmission <- factor( df$transmission )
df$transmission <- factor( df$transmission, levels = c("Manual","Semi-Auto","Automatic"),labels = paste0("f.Trans-",c("Manual","SemiAuto","Automatic")))

var_out<-calcQ(df$mileage)
llout_mil<-which((df$mileage<var_out$souti)|(df$mileage>var_out$souts))
iouts[llout_mil]<-iouts[llout_mil]+1
df[llout_mil,"mileage"] <- NA

df$fuelType <- factor(df$fuelType)
df$fuelType <- factor( df$fuelType, levels = c("Diesel","Petrol","Hybrid"), labels = paste0("f.Fuel-",c("Diesel","Petrol","Hybrid")))

var_out<-calcQ(df$tax)

llout<-which((df$tax <= var_out$souti & df$tax >= var_out$souts))
iouts[llout] <- iouts[llout]+1
jouts[which(colnames(df)=="tax")]<-length(llout)
df[llout, "tax"] <- NA

var_out<-calcQ(df$mpg)
llout_mpg<-which((df$mpg<var_out$souti)|(df$mpg>var_out$souts))
iouts[llout_mpg]<-iouts[llout_mpg]+1
jouts[which(colnames(df)=="mpg")]<-length(llout)
df[llout_mpg,"mpg"] <- NA

df$engineSize <- factor(df$engineSize)

sel<-which(df$engineSize==0)
ierrs[sel]<-ierrs[sel]+1 #Vector of errors per individual update

df[sel,"engineSize"]<-NA

library(missMDA)
vars_con<-c("year", "mileage", "tax", "mpg")
vars_res<-c("price", "Audi")

res.impca<-imputePCA(df[,vars_con],ncp=3)

df[,vars_con ]<-res.impca$completeObs
df$year<-round(df$year, digits=0)
df$mpg<-round(df$mpg, digits=2)

vars_dis<-c("model","transmission","fuelType","engineSize","manufacturer")
res.immca<-imputeMCA(df[,vars_dis],ncp=4)
df[ , vars_dis ]<-res.immca$completeObs


df$Audi<-ifelse(df$manufacturer == "Audi",1,0)
df$Audi<-factor(df$Audi,labels=c("No","Yes"))

df$years_after_sell <-  2022 - df$year

quants <- calcQ(df$tax)

df$f.tax<-factor(cut(df$tax, breaks=c(quants$min,quants$q1, quants$q2, quants$q3+10, quants$max), include.lowest=T))
df$f.tax <- paste0("f.tax-", df$f.tax)

df$f.mileage<-factor(cut(df$mileage,breaks=c(quantile(df$mileage,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))
df$f.mileage <- paste0("f.mil-", df$f.mileage)

df$f.mpg<-factor(cut(df$mpg,breaks=c(quantile(df$mpg,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))
df$f.mpg <- paste0("f.mpg-", df$f.mpg)

df$f.year<-factor(cut(df$year,breaks=c(quantile(df$year,seq(0,1,0.25),na.rm=TRUE)),include.lowest = T ))

df$inconsistencies <- imis+iouts+ierrs

res.mout <- Moutlier( df[ ,c(2,3,5,8)], quantile = 0.995) 
par(mfrow = c(1,1))
plot( res.mout$md, res.mout$rd )
abline( h=res.mout$cutoff, lwd=2, col="red") 
abline( v=res.mout$cutoff, lwd=2, col="red")
llmout <- which( ( res.mout$md > res.mout$cutoff ) & (res.mout$rd > res.mout$cutoff) )
df$mout <- 0 
df$mout[ llmout ] <- 1 
df$mout <- factor( df$mout, labels = c("MvOut.No","MvOut.Yes"))
```

# Principal Component Analysis
## Calculate total variance explained by each principal component
```{r, out.width="50%", out.height="50%"}
vars_con <- c("mileage", "tax", "mpg", "years_after_sell", "inconsistencies")
vars_dis <- c("transmission", "fuelType", "engineSize", "manufacturer")
vars_res <- c("price", "Audi")
res.pca<-PCA(df[,vars_con], ind.sup = llmout)
summary(res.pca)
```

According to the Kaiser criteria we should keep 2 dimensions, because it says that we should keep that dimensions with variance > 1.

## Elbow
```{r, out.width="50%", out.height="50%"}
fviz_screeplot(
  res.pca, 
  addlabels=TRUE, 
  ylim=c(0,50), 
  barfill="darkslateblue", 
  barcolor="darkslateblue",
  linecolor = "skyblue1"
)

```

As we have a not-so-ideal scree plot curve, we have to choose between a couple ways of deciding how many dimensions keep:
Kaiser rule: pick PCs with eigenvalues of at least 1.
Proportion of variance plot: the selected PCs should be able to describe at least 80% of the variance.

If we follow Kaiser rule we should keep only 2 dimensions, but considering that the 3rd dimensions variance is 0.965 (which is very close to 1) and that with only 2 dimensions we don't describe at least 80% of the variance, we have decided to keep 3 dimensions.

## Interpreting the axes
Variables point of view coordinates, quality of representation, contribution of the variables
### Axe 1
```{r}
res.dimdes <- dimdesc( res.pca, axes = 1:3, proba = 0.01)
res.dimdes$Dim.1
```

Dimension 1 has a great direct correlation with variables years_after_sell, mileage and mpg, and a great inverse correlation with variable tax.

### Axe 2
```{r}
res.dimdes$Dim.2
```

Dimension 2 has a great direct correlation with variable inconsistencies;
and a low inverse correlation with variables tax, years_after_sell and mileage

### Axe 3
```{r}
res.dimdes$Dim.3
```

Dimension 3 has a good direct correlation with variable inconsistencies and tax;
and a good inverse correlation with variables mpg.


## Individuals point of view
### Contribution
```{r, out.width="50%", out.height="50%"}
res.pca<-PCA(df[,vars_con], ind.sup = llmout)
#We perform a PCA using multivariate outliers as supplementary observations
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.40)
```

We can see that there are some individuals that are too contributive. So now, let s try to understand them better with extreme individuals.

### Extreme individuals

#### In dimension 1:

```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
```


```{r, out.width="50%", out.height="50%"}
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[955,1:19] #most contributive observation in positive 1st dimension
df[3327, 1:19] #most contributive observation in negative 1st dimension
```
The 1st observation, 955, has a very high mileage and mpg, is super cheap, has low taxes and was sold long ago.
The 2nd observation, 3327, is the opposite of the 1st. It has very low mileage, but it is extremly expensive and has very high taxes.


#### In dimension 2:

```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
```


```{r, out.width="50%", out.height="50%"}
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[3327,1:19] #most contributive observation in positive 2nd dimension
df[1396, 1:19] #most contributive observation in negative 2nd dimension
```

## Interpreting the axes: Variables point of view coordinates, quality of representation, contribution of the variables
```{r}
res.des <- dimdesc(res.pca)
```

### First dimension
```{r, out.width="50%", out.height="50%"}
fviz_contrib(  # contributions of variables to PC1
  res.pca, 
  fill = "darkslateblue",
  color = "darkslateblue",
  choice = "var", 
  axes = 1, 
  top = 5)
res.des$Dim.1
```

In the first dimension we see that for the quantitative variables the most positively related, from more to less, are:

* Years_after_sell (0.83)
* mileage (0.82)
* mpg (0.75) 

 Finally we have that variable tax is negatively correlated with a -0.71 cor value.

### Second dimension
```{r, out.width="50%", out.height="50%"}
fviz_contrib(  # contributions of variables to PC1
  res.pca, 
  fill = "darkslateblue",
  color = "darkslateblue",
  choice = "var", 
  axes = 2, 
  top = 5)
res.des$Dim.2
```

For the second dimension we see that the most positively correlated variable is "inconsistencies" with a value of 0.84, and negatively correlated is tax with a -0.31 value.

**We can conclude, then, that the first dimension is the one with the biggest correlations and that explains most of the data.**


## PCA taking into account also supplementary variables
Our supplementary variables are Price and Audi.
```{r, out.width="50%", out.height="50%"}
res.pca<-PCA(df[,c(vars_res,vars_con, vars_dis)], ind.sup = llmout, quanti.sup = 1, quali.sup = c(2,8:11))
```

The variable price is strongly negatively related with the first dimension axis. This means that it has a great correlation with variable tax and inverse correlation with variables mpg, mileage and years_after_sell. 

The variable Audi has very similar centroids.


# Kmeans Classification
We determined that we had to keep 3 dimensions of the PCA to maintain an inertia above 80%. Therefore we will have to use 3 dimensions for the computation of the Kmeans.

## Optimal number of clusters
```{r, out.width="50%", out.height="50%"}
fviz_nbclust(res.pca$ind$coord[,1:3], kmeans, method = "wss")
```
Using the elbow method, we determined that the optimal number of clusters to compute for kmeans was 5. With 5 clusters we retain sufficient inertia and also we keep the within distance small and the between distance big.

## Kmeans computation and visualization
```{r, out.width="50%", out.height="50%"}
set.seed(1)
#when setting a seed before the generation of kmeans we force kmeans to generate the same result (a cluster with an inertia > 0.75)
res.km<-kmeans(res.pca$ind$coord[,1:3],5)

res.km$betweenss/res.km$totss   #calculate total retained inertia
table(res.km$cluster)
ff<-factor(res.km$cluster)
plot(res.pca$ind$coord[,1:3],col=ff, pch=19, main= "K-Means - 5 cluster - First Factorial Plane")
legend("bottomleft", title="K-Means", legend=levels(ff), col=1:5, pch=19, cex=0.8)
```

### Gain in inertia (in %)
After executing kmeans we get a retained inertia of the 78.88098%
```{r}
100*(res.km$betweenss/res.km$totss)
```

## Profiling of clusters
```{r, out.width="50%", out.height="50%"}
df$kmeans_clust <-0
k=5
df[-llmout,"kmeans_clust"]<-res.km$cluster
df[llmout,"kmeans_clust"]<-k+1
df$kmeans_clust <- factor(df$kmeans_clust)
# observations that are multivariant outliers will be put in cluster 6
barplot(table(df$kmeans_clust),col="darkslateblue",border="darkslateblue",main="[k-means]#observations/cluster")

res.cat <- catdes(df[c(2:12, 18:20)], num.var=14, proba=0.01)
```

We proceed to explain the data obtained.

### Description of clusters by categorical variables

```{r}
res.cat$test.chi2
```

We start with the description of the categorical variables that characterize the clusters, so in this output we do
not have dimensions because it is the total association. We can see the intensity of the variables, in our case the variables that affect more to the clustering are **fuelType**, **engineSyze** and **f.price**, because are the one with the smallest p.value.
We excluded from this test the factor variables that resulted from the grouping of the quantiles of the numerical variables because we will analyze these numerical variables later on. This way we reduce redundant information.

Next, we want to see for each cluster which are the categories that characterize them.

```{r}
res.cat$category
```

* Cluster 1
  + The first thing we can notice is that cluster 1 contains 28.7% of all cars with engineSize=3 in the sample. On average 11.42% of the cars have an engine of that size, but in cluster 1 those cars are overrepresented (29.98%). In addition, cheap cars are overrepresented (20% of global mean vs 32.5% mean in cluster 1). Furthermore, automatic cars are overrepresented (26.4% of global mean vs 39.3% mean in cluster 1). Finally, the cluster has very few expensive and extremly expensive cars.
  
* Cluster 2
  + The first thing we can notice is that cluster 2 contains 71.21% of hybrid fuel cars in the sample. On average 1.32% of the cars use hybrid fuel, but in cluster 2 hybrid cars are overrepresented (57.32%). Diesel cars represent 57.2% of the sample, but 0.87% of them are included in Cluster 2. The same can be seen for cars that run on petrol. Cars with an engineSize of 1 represent 7.48% of the sample, but 0% of them are included in Cluster 1. Finally, on average 26.4% of the cars use an automatic transmission, but in cluster 2 automatics cars are overrepresented (57.32%) and manual and semi-automatic cars are underrepresented.
  
* Cluster 3
  + The first thing we can notice is that cluster 3 contains 82.6% of all extremly expensive cars in the sample. On average 19.9% of the cars are extremly expensive, but in cluster 3 those cars are overrepresented (41.54%). In addition, very expensive cars are overrepresented (20% of global mean vs 34.5% mean in cluster 3). Furthermore, semi-automatic cars are overrepresented (38.78% of global mean vs 49.62% mean in cluster 3). Finally, manual, cheap and super cheap cars are underrepresented in the class.
  
* Cluster 4
  + In cluster 4 we can see that there is more expensive cars than in the gobal (38.3951682% in cluster 4 vs. 19.98% in global). It happens the same with cheap cars (25.6255393% in cluster 4 vs. 20.04% in global). But for the super cheap and extremely expensive cars is the opposite, they are underrepresented. For the extremely expensive cars the global percentage is 19.90%, while the percentage in cluster 4 is 5.4357204%, and for super cheap cars the global percentage is 20.00% and the cluster 4 percentage is 10.8714409%. So we can say that cluster 4 has the cars with an average price similar to the mean.
  
* Cluster 5
  + Cluster 5 is defined by the cheapest cars. We can confirm this it contains a great percentage of super cheap and cheap cars and a very little percentage of extremely expensive and very expensive cars. The percentage of super cheap cars in cluster 5 is 53.51% while in the global is 20.00%, and the percentage of cheap cars in cluster 5 is 34.89% while in the global is 20.04%. We can observe the opposite behaviour with the expensive cars: the percentage of extremely expensive cars in cluster 5 is 1.12% while in the global is 20.08%, and the percentage of cheap cars in cluster 5 is 0% while in the global is 19.90%. We can also highlight that it has a lot of cars with manual transmission (61.83% in cluster 4 vs. 34.82% in global).

* Cluster 6
  + This cluster groups all multivariant outliers in the sample. What we can observe from this cluser is that contains extreme observations like super cheap and extremely expensive cars and cars with a very high engine size. Also, cars with automatic transmission and that are BMW are overrepresented while semi-auto, expensive and very expensive cars are underrepresented.


We now proceed to see the quantitative variables that characterizes the clusters. We can see in the output from res.cat$quanti.var all the numeric variables that characterize the clusters. From a more detailed look, variables **year**, **mileage**, **tax** and **inconsistencies** maintain a strong relation with the cluster number because of their high eta2 value.
```{r}
res.cat$quanti.var
res.cat$quanti
```


We want to know now which variables are associated with the quantitative variables.

* Cluster 1
  + We can see that for cluster 1 we have cars that have a lot of mileage (average mean of class of 47044 vs 23289 overall mean) and cars with high taxes (60 units over the overall mean). The cluster contains cheaper cars than the global mean (4230 units cheaper on average). It also contains cars with lower mpg and cars that were sold on before the average selling year of the dataset. Finally the cluster does not cocntain any obervation with an inconsistency.
  
* Cluster 2
  + For cluster 2, we have cars with a higher mileage than the global average (30203 mean in cluster 2 vs 23289 global mean). We also have cars that have lower taxes. This cluster groups almost all observations with inconsistencies. Some other observations with inconsistencies are in cluster 6.
  
* Cluster 3
  +  This cluster gropus cars that were sold in 2019 on average. The cars also have a higher average price (6603 units over the global mean). In addition, these cars have lower taxes. Finally, we do not have any observation with an inconsistency.
  
* Cluster 4
  + In cluster 4 we can't see a great difference between global mean and the mean of the cluster but we can highlight that cars in cluster 4 have more mpg and taxes than in the global mean. The global mean of tax is 122.91 and its mean in cluster 5 is 141.33, and the global mean of mpg is 53 and its mean in cluster 5 is 57.46.
  
* Cluster 5
  + In cluster 5 we can highlight that the price of the cars is very cheap, the mean of the cluster is 12808.48 while the global mean is almost double: 21418.53. It is also very remarkable that the taxes are very low, the mean of the cluster is 22.61 while the global mean is almost double: 122.91. This variables have lower mean that the global but we also have variables that highlight for having a higher mean, like mpg and mileage. The global mean of mpg is 53 and its mean in cluster 5 is 65.27, and the global mean of mileage is 23289.52 and its mean in cluster 5 is 38085.44.
  
* Cluster 6
  + In cluster 6 we can see that the inconsistencies are higher that in the overall mean, as well as the mileage, tax and price:
                 Mean in category Overall mean sd in category
inconsistencies  0.3597561        0.02960
mileage          59000.55         23289.51910
tax              172.3171         122.91100
price            29204.11         21418.53580

The variable mean mpg in cluster 6 (44.96) is lower than the overall mean (53), and the cars in cluster 6 are older because the mean of the variable year is 2014 and in the overall mean is 2017.

## HCPC computation and visualization
```{r, out.width="50%", out.height="50%"}
#dis<-dist(res.pca$ind$coord[,1:2])
res.hcpc <- HCPC(res.pca,nb.clust = -1, proba = 0.01)
100*(res.hcpc$call$t$within[1]-res.hcpc$call$t$within[1:10])/(res.hcpc$call$t$within[1])   #calculate total retained inertia
# Individuals facor map
fviz_cluster(res.hcpc, geom = "point", main = "Factor map")
```

### Gain in inertia (in %)
After executing HCPC we get a retained inertia of the 63.88594%
```{r}
100*(res.hcpc$call$t$within[1]-res.hcpc$call$t$within[4])/(res.hcpc$call$t$within[1])
```

## Profiling of clusters
```{r, out.width="50%", out.height="50%"}
df$HCPC_clust <-0
k=4
df[-llmout,"HCPC_clust"]<-res.hcpc$data.clust$clust
df[llmout,"HCPC_clust"]<-k+1
df$HCPC_clust <- factor(df$HCPC_clust)
# observations that are multivariant outliers will be put in cluster 6
barplot(table(df$HCPC_clust),col="darkslateblue",border="darkslateblue",main="[HCPC]#observations/cluster")
```

We proceed to explain the data obtained.

### Description of clusters by categorical variables

```{r}
res.hcpc$desc.var$test.chi2
```

We can see the intensity of the variables, in our case the variables that affect more to the clustering are **fuelType**, **engineSize**,**transmission** and **manufacturer**  because are those ones with the smallest p.value.
We excluded from this test the factor variables that resulted from the grouping of the quantiles of the numerical variables because we will analyze these numerical variables later on. This way we reduce redundant information.

Next, we want to see for each cluster which are the categories that characterize them.

```{r}
res.hcpc$desc.var$category
```

* Cluster 1
  + The first thing we can notice is that cluster 1 contains the 60.052632% of all the cars with semi-automatic transmission. The 49.84894260% of its cars use petrol while the global percenatge of cars that use petrol is 41.4598842%, that means that is overrepresented. 
  
* Cluster 2
  + In cluster 2 we can see that The 65.28497409% of its cars use diesel while the global percenatge of cars that use petrol is 57.2373863%, that means that is overrepresented. The manufacturer BMW as well as cars with automatic transmission are overrepresented.
  
* Cluster 3
  + Cluster 3 contains 74.6% of all hybrid cars from the data set. On average 1.3% of the cars use hybrid fuel, but in cluster 3 hybrid cars are overrepresented (57.32%). Automatic cars are also overrepresented in the class with a 57.3% in comparison to a 25.8% global mean. BMW cars are also overrepresented in cluster 3 with a 41.46% in comparison to a 21.2% global mean.

* Cluster 4
  + Cluster 4 contains 40.2% of all manual cars from the data set. On average 34.8% of the cars are manual, but in cluster 4 manual cars are overrepresented (62.3%). Automatic cars are also overrepresented in the class with a 57.3% in comparison to a 25.8% global mean. VW cars are overrepresented where on the other hand, Mercedes and BMW cars are underrepresented.

We now proceed to see the quantitative variables that characterizes the clusters. We can see in the output from **res.hcpc$desc.var$quanti.var** all the numeric variables that characterize the clusters. From a more detailed look, variables **years_after_sell**, **mileage**, **tax** and **inconsistencies** maintain a strong relation with the cluster number.
```{r}
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

We want to know now which variables are associated with the quantitative variables.

* Cluster 1
  + Cluster 1 groups cars with higher prices than the overall mean. The mean price in this first cluster is 27024 while the overall mean sits at 21154. It also contains cars that pay higher taxes and cars that have higher fuel consumption (low mpg). Finally, the cluster contains cars with a very low mileage (7200 on avg) compared to the global mean of 22000 and the cars where sold recently, as they average year after sell is 3 for the cluster and 4.7 for the global mean.

* Cluster 2
  + Cluster 2 contains cars that where sold earlier than other cars, as they average year after sell is 6 for the cluster and 4.7 for the global mean. It also contains cars with a higher mileage and higher tax. Finally it contains cheaper cars than the global mean, 17850 dollars for the cluster vs 21100 for the overall mean.
  
* Cluster 3
  + In this cluster we can see that the variable mileage is higher than the overall mean while the taxes are lower. This can indicate us that they are old or very used cars. The most important trait for this cluster is that it groups observations that only have inconsistencies. The price is not highlighted so it must be arround the global mean. 
  
* Cluster 4
  + Cars in cluster 4 have the biggest mileage mean, being it 37241.21087. Its cars spend also less fuel than the overall mean. It is very important to highlight that the cars in cluster 4 are very cheap and have very low taxes.

# CA analysis
CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors.

## CA: f.price vs f.tax

We start the CA with the creation of a contingency table and a Pearson's chi2 test to test for independence.
```{r}
tt<-table(df[,c("f.price","f.tax")])
tt
chisq.test(tt) #to see if variables are independents. H0: Variables are independent
```
We get a p-value lower than 0.05 so we can reject the H0. In our sample, the row and the column variables are statistically significantly associated.

We are now going to take a look to the simple correspondences. 
```{r, out.width="50%", out.height="50%"}
res.ca <- CA(tt)
summary(res.ca,dig=2)
fviz_eig(res.ca) #Visualize the eigenvalues
```
According to the Kaiser criteria, we have to consider the first dimension. In this correspondence analysis almost all the variance is explained in the 1st dimension (96.74%).

```{r, out.width="50%", out.height="50%"}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
legend("bottomleft", title="Lines", legend=c("Tax", "Price"), col=c("red","blue"), pch=19, cex=0.8)
```
This graph shows that with mildly high taxes come cars with prices that are average or high. With cheap cars come very low taxes. Very high taxes come with cars that are between expensive and cheap. With extremely expensive cars we get mildly high taxes. 

### Contribution of rows to the dimensions
```{r, out.width="50%", out.height="50%"}
res.ca$row$contrib
fviz_contrib(res.ca, choice = "row", axes = 1:2, top = 10)
```
The row variables with the larger value, contribute the most to the definition of the dimensions.
Rows that contribute the most to Dim.1 are the most important in explaining the variability in the data set. In our case, the most important row categories for price are super cheap & extremely expensive cars.
Rows that do not contribute much to any dimension or that contribute to the last dimensions (Dim 2) are less important. In our case these are cheap & expensive cars.

The most important (or, contributing) row points can be highlighted on the scatter plot as follow:
```{r, out.width="50%", out.height="50%"}
fviz_ca_row(res.ca, col.row = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)
```
The scatter plot gives an idea of what pole of the dimensions the row categories are actually contributing to. It is evident that the row category "super cheap" has an important contribution to the positive pole of the first dimension, while the category "extremely expensive" has a major contribution to the negative pole of the first dimension.

In other words, dimension 1 is mainly defined by the opposition of super cheap cars (positive pole), and extremely expensive cars (negative pole).


## CA: f.price vs fuelType

We start the CA with the creation of a contingency table and a Pearson's chi2 test to test for independence.
```{r}
tt<-table(df[,c("f.price","fuelType")])
tt
chisq.test(tt) #to see if variables are independents. H0: Variables are independent
```
We get a p-value lower than 0.05 so we can reject the H0. In our sample, the row and the column variables are statistically significantly associated.

We are now going to take a look to the simple correspondences. 
```{r, out.width="50%", out.height="50%"}
res.ca <- CA(tt)
fviz_eig(res.ca) #Same outputs as PCA
summary(res.ca,dig=2)
```
According to the Kaiser criteria, we have to consider the first dimension. In this correspondence analysis almost all the variance is explained in the 1st dimension (97.1%).

```{r, out.width="50%", out.height="50%"}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
legend("bottomleft", title="Lines", legend=c("FuelType", "Price"), col=c("red","blue"), pch=19, cex=0.8)
```
Cars that use petrol fuel are priced in an undetermined matter, because its point in the CA is placed between super cheap and very expensive cars. The same happens to diesel cars because they are placed between cheap and extremely expensive cars. Finally hybrid cars appear to be very far from all other cars. In conclusion, this graph does not provide very conclusive information to correlate the price of the cars with its type of fuel.


### Contribution of rows to the dimensions
```{r, out.width="50%", out.height="50%"}
res.ca$row$contrib
fviz_contrib(res.ca, choice = "row", axes = 1:2, top = 10)
```
The row variables with the larger value, contribute the most to the definition of the dimensions.
Rows that contribute the most to Dim.1 are the most important in explaining the variability in the data set. In our case, the most important row categories for price are super cheap cars & cheap cars.
Rows that do not contribute much to any dimension or that contribute to the last dimensions (Dim 2) are less important. In our case these are expensive & very expensive cars.

The most important (or, contributing) row points can be highlighted on the scatter plot as follow:
```{r, out.width="50%", out.height="50%"}
fviz_ca_row(res.ca, col.row = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)
```
The scatter plot gives an idea of what pole of the dimensions the row categories are actually contributing to. It is evident that the row category "cheap" has an important contribution to the positive pole of the first dimension, while the category "super cheap" has a major contribution to the negative pole of the first dimension.

In other words, dimension 1 is mainly defined by the opposition of cheap cars (positive pole), and super cheap cars (negative pole).


# MCA Analysis


```{r, out.width="50%", out.height="50%"}
par(mfrow=c(1,1))
vars_dis <- c("model","transmission","fuelType","engineSize","manufacturer","f.price","f.tax","f.mileage","f.mpg","f.year")
res.mca<-MCA(df[,c(vars_dis[c(2,5:10)])])
#summary(res.mca,nbelements=50, nbind=0)
```

## Interpreting the axes association to factor map

### Variables representation graph (3rd graph from MCA)
Correlation between variables and principal dimensions:
From the Variables representation map, we can see that all variables are placed on the first quadrant of the first factorial plane. From a more detailed look, the manufacturer variable has no importance on the 1st dimension but explains the 2nd dimension a bit, so cars will be scattered according to its manufacturer alongside the 2nd dimension. Transmission explains the 1st and 2nd dimension in a equal fashion, so the cars will be scattered alongside the 2 dimensions. Variables f.mpg, f.mileage, f.price, f.year and f.tax have a high representation on the positive side of the 1st dimension, so cars will be scattered alongside the 1st dimension according to these variables.

### MCA factor map
```{r, out.width="50%", out.height="50%"}
fviz_mca_var(res.mca, col.var="contrib",
             gradient.cols =rainbow(7) , 
             repel = TRUE, # avoid text overlapping (slow)
             ggtheme = theme_minimal()
             )
```
From this graph we can conclude that, Audi, Mercedes and BMW cars do not have a strong correlation with any factor variable because they are more or less centered and far away from the center of any other variable. As BMW and Mercedes are close together we could say that they have similar qualities. The most relevant groups are:
 * Very expensive cars with cars sold between 2017-2019 and taxes between 125-145.
 * VW and cars sold between 2016-2017
 * Cars sold in 2008-2016 and very high mileage.


## Eigenvalues and dominant axes analysis

**How many axes we have to consider for next Hierarchical Classification stage?**

```{r}
mean(res.mca$eig[,1])
head(get_eigenvalue(res.mca), 10)
```
We consider, according to the generalized Kaiser theorem, all those dimensions such that their eigenvalue is greater than the mean. We see that the average gives us 0.1428571. Therefore, we will take up to dimension 8, which represents the 60.88943% of the sample.

We can also visualize the percentages of inertia explained by each MCA dimensions:
```{r, out.width="50%", out.height="50%"}
fviz_screeplot(
  res.mca, 
  addlabels=TRUE, 
  ylim=c(0,20), 
  barfill="darkslateblue", 
  barcolor="darkslateblue",
  linecolor="skyblue1"
)
```

## Individuals point of view
Are they any individuals "too contributive"?
```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(
  res.mca, 
  geom=c("point"),
  col.ind="contrib", 
  gradient.cols=c("darkslateblue", "red")
)
```
We can see that individuals located in the periphery are the most contributive. 


### Groups of individuals
```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="transmission")
```
Automatic cars are located on the positive side of the 2nd dimension where manual cars are on the nagative side of the 2nd dimension.

```{r,out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="manufacturer")
```
We cannot see any significant difference in the position of observations.

```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="f.price")
```
Individuals are distributed across the 1st dimension: from extremely expensive cars on the negative side to super cheap on the positive side.

```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="f.tax")
```
We can only appreciate that observations with very high taxes are located on the positive side of the 2nd dimension.

```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="f.mileage")
```
Individuals are distributed across the 1st dimension: from very low mileage on the negative side to very high mileage on the positive side.

```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="f.mpg")
```
We cannot see any significant difference in the position of observations.


```{r, out.width="50%", out.height="50%"}
fviz_mca_ind(res.mca, label="none", habillage="f.year")
```
Individuals are distributed across the 1st dimension: from an early time of selling (high year) on the negative side to very old year of selling (low year) on the positive side.


```{r, out.width="50%", out.height="50%"}
fviz_ellipses(res.mca, c("f.price", "f.year"),
              geom = "point")
```


## MCA using multivariant
**How do supplementary variables enhance the axis interpretation?**

```{r, out.width="50%", out.height="50%"}
llvout<-which(df$mout=="MvOut.Yes");length(llvout) #Multivariate outliers
res.mca<-MCA(df[,c(vars_dis[c(2:3,5:10)],"price", "inconsistencies", "Audi")], quali.sup=c(11),quanti.sup=c(9:10), ind.sup=llvout)
mean(res.mca$eig[,1])
head(get_eigenvalue(res.mca), 10) #keep 9 dimensions
fviz_mca_var(res.mca, col.var="contrib",
             gradient.cols =rainbow(7) , 
             repel = TRUE, # avoid text overlapping (slow)
             ggtheme = theme_minimal()
             )

```
From the supplementary quantitative variables graph we can see that variable price will explain a lot of the negative 1st dimension, so cars with a low price will be located on the positive part of the 1st dimension and expensive cars on the negative side of the dimension. 
In addition, when we add the multivariate outliers as supplementary individuals, they get taken out of the mca computation so they do not disrupt the calculus with their extreme values.
Now, when we draw a variable factor map, we can also see the position of our supplementary variables relative to our original categorical variables and see how they correlate to each other. For example, from the previous graph we can see that our binary target variable "Audi", does not correlate with any variable whatsoever, as its 2 main values "Yes" and "No" are located on the center of the graph. We also saw this in the previous MCA analysis, when the Audi point of variable manufacturer were located on the center of the graph.


# Hierarchical Clustering from MCA

```{r, fig.show='hide', out.width="50%", out.height="50%"}
res.hcmc<-HCPC(res.mca,nb.clust=-1,order=TRUE)
res.hcmc$call$t$within[1:15]
(res.hcmc$call$t$within[1]-res.hcmc$call$t$within[1:10])/res.hcmc$call$t$within[1]
```

We consider that 6 is the better number of cluster to have because it contains the 64.2% of the variability and having more clusters doesn't increases significantly the amount of variability. 

```{r, out.width="50%", out.height="50%"}
res.hcmc<-HCPC(res.mca,nb.clust=6,order=TRUE)
df$claHCMC<-7
df[row.names(res.hcmc$data.clust),"claHCMC"]<-res.hcmc$data.clust$clust
df$claHCMC<-factor(df$claHCMC)
table(df$claHCMC)
#Multivariate outliers will be in cluster 7

# Interpret clustering results
barplot(table(df$claHCMC),col="darkslateblue",border="darkslateblue",main="[HCPC]#observations/cluster")

# Individuals factor map
fviz_cluster(res.hcmc, geom = "point", main = "Factor map")

```


## Description of clusters by categorical variables

We proceed to explain the data obtained.

```{r}
res.hcmc$desc.var$test.chi2
```

We can see the intensity of the variables, in our case the categorical variables that affect more to the clustering are **f.price**, **f.tax**, **f.mileage**, **f.mpg** and **f.year**  because are those ones with the smallest p.value.
Next, we want to see for each cluster which are the categories that characterize them.

```{r}
res.hcmc$desc.var$category
```

* Cluster 1
  + We can see that cars in the first cluster are extremely expensive and sold in the year 2020. Moreover, they have low mpg and mileage. Audi and BMW cars are overrepresented.

  
* Cluster 2
  + Cars in cluster 2 use to have been sold between 2018 and 2019. Very expensive cars and with low mileage are overrepresented. The avegare amount of Mercedes cars is a little bit larger than the global average (overrepresented). 
  
    
* Cluster 3
  + Cars in cluster 3 use to have been sold in 2017. They have a mileage close to the mean and half of them are expensive cars (they are overrepresented).


* Cluster 4
  +  Cars in cluster 4 use to have been sold between 2008 and 2016. They have higher taxes and mileage than the mean, as well as lower miles per galon. Automatic and BMW cars are overrepresented.
  
  
* Cluster 5
  +  Cars in cluster 5 use to have been sold between 2008 and 2016. They are characterized for having manual transmission and using petrol. Besides, they have low taxes and are super cheap cars, and use to be VW.

  
* Cluster 6
  + Cars in cluster 6 use to have been sold between 2008 and 2016. They are characterized for using diesel, having high miles per galon and very high mileage. Besides, they have low taxes and cheap cars are overrepresented.

  
We now proceed to see the quantitative variables that characterizes the clusters. 

```{r}
res.hcmc$desc.var$quanti.var
```

The quantitative variable that only characterizes the clusters is **price**, as variable "inconsistencies" has a very low eta2 value. It has to be told that they are the only quantitative variables included in the MCA as supplementary.

```{r}
res.hcmc$desc.var$quanti
```

* Cluster 1
  + High price, 11846 dollars more than the average price.
  
* Cluster 2
  + Price 3782 dollars higher than the average price.
  
* Cluster 3
  + Price 2035 dollars lower than the average price.

* Cluster 4
  + Price very similar to the average price.
  
* Cluster 5
  + Price 10450 dollars lower than the average price.
  
* Cluster 6
  + Price 6567 dollars lower than the average price.

## C. The description of the clusters by the individuals
```{r}
res.hcmc$desc.ind$para
```

This command allow us to see for each cluster the top 5 closest individuals to the cluster center. Below each individual it's the distance between each individual and the cluster center. 

We are gonna see the values of the variables of these individuals and search for a correlation with the characteristics that describes our clusters.

Cluster 1:
```{r}
summary(df[c(3451,4851,4968,38,4127),])
```

We can see that all of them were sold in 2020. They use to have semiautomatic transmission and use diesel. Besides, they are or extremely expensive or expensive cars.

Cluster 2:
```{r}
summary(df[c(3789,3922,4148,4156,4157),])
```

We can see that all of them were sold in 2019. They use to have automatic transmission and use diesel. It can be also highlighted that they have very low mileage, so they little-used cars. Besides, they are expensive cars and all of them are VW.

Cluster 3:
```{r}
summary(df[c(3938,4663,184,476,4556),])
```

We can see that all of them were sold in 2017. They have automatic or semiautomatic transmission and use diesel. It can be also highlighted that they have high mileage, so they are very used cars. Besides, they are expensive cars.

Cluster 4:
```{r}
summary(df[c(4858,4860,4863,4842,4874),])
```

We can see that all of them were sold in 2015 or 2016. They have automatic or semiautomatic transmission and use diesel. It can be also highlighted that they have high mileage, so they are very used cars. Besides, they are expensive or very expensive cars, and all of them are VW.

Cluster 5:
```{r}
summary(df[c(3603,3874,4171,4174,689),])
```

We can see that all of them were sold in 2015 or 2016. They use to have semiautomatic transmission and use petrol. It can be also highlighted that they have very low taxes. Besides, they are super cheap cars.

Cluster 6:
```{r}
summary(df[c(3191,2225,2320,2491,2568),])
```

We can see that all of them were sold in 2013, 2015 or 2016. They use to have semiautomatic transmission and use diesel. It can be also highlighted that they have high mileage, so they are very used cars. Besides, they use to be cheap cars, and all of them are Mercedes.



## Comparison of clusters obtained after K-Means (based on PCA)
In general, we get similar results from our Kmeans using PCA and from our HCMC. From our HCMC we get 5 clusters characterized by the price, year of selling and mileage of the cars where in the Kmeans we get 3 clusters where those variables are also a main quality. From our HCMC we also get a cluster characterized by high taxes and mileage and low mpg (cluster 4). Low miles per gallon means high consumption so therefore that is why we could see higher taxes in those cars. In contrast we did not see such cluster in the Kmeans, but we did see a cluster, for example, that was characterized by its transmission, fuel and type of engine (cluster 2). 



