hist(df$GrAppv)
####################
#SBA_Appv
summary(df$SBA_Appv)
hist(df$SBA_Appv)
#Check if extreme outliers
boxplot(df$SBA_Appv, main="Boxplot of SBA_Appv", col="darkslateblue")
summary(df$SBA_Appv)
var_out<-calcQ(df$SBA_Appv)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
llout<-which((df$SBA_Appv >= var_out$souts))
iouts[llout] <- iouts[llout]+1
jouts[which(colnames(df)=="SBA_Appv")]<-length(llout)
#Treat extreme outliers
#MEDIA GENERAL O MEDIA PARECIDOS? (de momento inputamos media)
df$SBA_Appv[which((df$SBA_Appv >= var_out$souts))] <- NA
summary(df$SBA_Appv)
hist(df$SBA_Appv)
#################
summary(df)
# IMPUTATION By THE 1NN
library(class)
# FOR EVERY INDIVIDUAL WITH MISSING Incomes LOOK FOR THE MOST SIMILAR INDIVIDUAL
# wrt REMAINING VARIABLES AND COPY THE VALUE OF INGRESSOS ON THE FIRST
#For more robustness average the values of k-NN in general (with small k)
#For several Variables:
#built indexes of numerical variables that require inputation
uncompleteVars <- c(4,5,7,8,11,13,14)
summary(df)
fullVariables <- c(3,16)
aux<-df[,fullVariables]
dim(aux)
names(aux)
for (k in uncompleteVars){
aux1 <- aux[!is.na(df[,k]),]
dim(aux1)
aux2 <- aux[is.na(df[,k]),]
dim(aux2)
RefValues<- df[!is.na(df[,k]),k]
#Find nns for aux2
knn.values = knn(aux1,aux2,RefValues)
#CARE: neither aux1 nor aux2 can contain NAs
#CARE: knn.ing is generated as a factor
#Be sure to retrieve the correct values
df[is.na(df[,k]),k] = as.numeric(as.character(knn.values))
fullVariables<-c(fullVariables, k)
aux<-df[,fullVariables]
}
mean(df$SBA_Appv)
#144820.1 (Antes de Knn y NAs)
#aprox 115000 (Despues de Knn)
dim(df)
summary(df)
View(df)
library(FactoMineR)
temperature <- read.table("http://factominer.free.fr/book/temperature.csv",
header=TRUE,sep=";",dec=".",row.names=1)
#acf nos sirve para ver si hay correlación entre los diferentes valores dentro de una columna
#si hubiera correlación implica que siguen un cierto orden
acf( temperature$Annual )
library(lmtest)
with(temperature, dwtest(Annual~1) )
with(temperature, dwtest(Annual~Area) )
summary( temperature )
res.cat <- catdes( temperature,17 ) # targe Area
#como el p-value de agosto es < 0.05 se rechaza la hipotesis nula
#h0 = media de temperatura en agosto es igual en todas las areas
res.cat$quanti.var
#Quines arees son/tenen una mitjana de temperatura diferent a una mitjana global?
res.cat$quanti
res.con <- condes( temperature, 13) # Target annual temperature
attributes( res.con )
#relacion con las variables qualitativas
res.con$quali
#relaciona con las variables quantitativas
res.con$quanti
#indica la relacion especifica de cada valor que pueden coger las categoricas
res.con$category
# Capitals are active rows - Jan to Dec variables actives
res<-PCA(temperature,ind.sup=24:35,quanti.sup=13:16,quali.sup=17)
# Capitals are active rows - Jan to Dec variables actives
res<-PCA(temperature,ind.sup=24:35,quanti.sup=13:16,quali.sup=17)
round(cor(temperature[,1:12]), digits = 2)
eigen(cor(temperature[,1:12]))
res<-PCA(temperature,ind.sup=24:35,quanti.sup=13:16,quali.sup=17)
summary(res, nb.dec = 2, ncp = 4, nbelements = 23 )
res.pca <- PCA(temperature[1:23,],scale.unit=TRUE,ncp=2,quanti.sup=13:16,quali.sup=17)
mean(res.pca$eig[,1])  # Kaiser valor propi > 1
sum(res.pca$eig[,1])
summary(res.pca)
res.pca
res.hcpc <- HCPC(res.pca,nb.clust = -1)
names(res.hcpc)
summary(res.pca)
res.pca
res.hcpc <- HCPC(res.pca,nb.clust = -1)
#Description of clusters
res.hcpc$desc.var  # Target clust - catdes()
res.hcpc$desc.var$test.chi2
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$category
res.hcpc$desc.var$quanti
# Components
res.hcpc$desc.axe
#Paragons and Specic Individuals
res.hcpc$desc.ind
res.hcpc$call$t$within  # TTS = WithSS + BetwSS   1 clust - TTS=WithSS   2clust TTS=5.05+6.75   3 Clusters TTS= 2.72 + (6.75+2.32)     23 Clust TTS = BetwSS
100*(res.hcpc$call$t$within[1]-res.hcpc$call$t$within)/(res.hcpc$call$t$within[1])
res.hcpc$call$t$inert.gain
kc<-kmeans(dist(res.pca$ind$coord[,1:2]),centers=4)
kc
kc$cluster
kc$withinss
kc$tot.withinss
kc$betweenss
kc$totss
100*kc$betweenss/kc$totss
table(kc$cluster,res.hcpc$data.clust$clust)
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr", "lmtest","effects", "statmod", "DescTools","ResourceSelection","chemometrics","missMDA")
#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
#verify they are loaded
search()
# Clear plots
if(!is.null(dev.list())) dev.off()
# Clean workspace
rm(list=ls())
load("~/GitHub/adei/adei/deliverable3/5000cars_clean.RData")
# 80% train sample and 20% test sample
llwork <- sample(1:nrow(df),round(0.80*nrow(df),0))
dfall<-df
df_train <- dfall[llwork,]
df_test <-dfall[-llwork,]
vars_con
ll<-which(df_train$tax==0);ll
df$tax[ll]<-0.5
m1<-glm(Audi~price+mileage+tax+mpg+years_after_sell,family="binomial",data=df_train)
summary(m1)
vif(m1)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10))
marginalModelPlots(m1)
library(effects)
plot(allEffects(m1))
crPlots(m1,id=list(method=cooks.distance(m1),n=5))
m2 <- glm(Audi~poly(price, 2)+poly(mileage,2)+poly(tax,2)+poly(mpg,2)+poly(years_after_sell,2),family="binomial",data=df_train)
summary(m2)
anova(m2, m1, test="LR")
AIC(m1,m2)
m3<-glm(Audi~poly(price,2)+poly(mileage,2)+poly(tax,2)+poly(mpg,2)+poly(years_after_sell,2),family="binomial",data=df_train[!df_train$mout=="MvOut.Yes",])
summary(m3)
m3<-glm(Audi~poly(price,2)+poly(mileage,2)+poly(tax,2)+poly(mpg,2)+poly(years_after_sell,2),family="binomial",data=df_train[!df_train$mout=="MvOut.Yes",])
summary(m3)
m4 <- step(m3)
summary(m4)
m5 <- glm(formula = Audi ~ poly(price, 2) + poly(mileage, 2) + tax + mpg, family = "binomial", data = df_train[!df_train$mout == "MvOut.Yes",])
summary(m5)
anova(m5, m4, test="LR")
AIC(m1, m2, m3, m4, m5)
table(df_train$engineSize)
df_train$engineSize_f <- as.numeric(levels(df_train$engineSize))[df_train$engineSize]
par(mfrow=c(1,1))
hist(df_train$engineSize_f)
quantile(df_train$engineSize_f, c(0.3333333,0.6666666,1))
df_train$engineSize_f <- factor(cut(df_train$engineSize_f, breaks = c(0,1.6,2,10)))
df_test$engineSize_f <- as.numeric(levels(df_test$engineSize))[df_test$engineSize]
df_test$engineSize_f <- factor(cut(df_test$engineSize_f, breaks =  c(0,1.6,2,10)))
table(df_train$engineSize_f)
m6 <- update(m5, ~.+fuelType+transmission+engineSize_f,data=df_train[!df_train$mout=="MvOut.Yes",])
vif(m6)
summary(m6)
avPlots(m5,id=list(method=hatvalues(m6),n=5))
crPlots(m5,id=list(method=cooks.distance(m6),n=5))
# library(effects)
plot(allEffects(m6), selection = 5)
plot(allEffects(m6), selection = 4)
plot(allEffects(m6), selection = 6)
m7 <- update(m6, ~.*(fuelType+transmission+engineSize_f)^2,data=df_train[!df_train$mout == "MvOut.Yes",])
summary(m7)
anova(m7,m6, test="LR")
Anova(m7, test="LR")
m8<- step(m7)
summary(m8)
anova(m8, m7, test="Chisq")
AIC(m1, m2, m3, m4, m5, m6, m7, m8)
summary(m8)
m7 <- update(m6, ~.*(fuelType+transmission+engineSize_f)^2,data=df_train[!df_train$mout == "MvOut.Yes",])
summary(m7)
m6 <- update(m5, ~.+fuelType+transmission+engineSize_f,data=df_train[!df_train$mout=="MvOut.Yes",])
summary(m6)
m7 <- update(m6, ~.*(fuelType+transmission+engineSize_f)^2,data=df_train[!df_train$mout == "MvOut.Yes",])
summary(m7)
m8<- step(m7)
summary(m8)
anova(m8, m7, test="Chisq")
summary(m9)
summary(m8)
m9 <- glm(formula = Audi ~ poly(price, 2) + poly(mileage, 2) + tax +
mpg + fuelType + transmission + engineSize_f + fuelType:transmission +
fuelType:engineSize_f + poly(price,
2):fuelType + poly(mileage,
2):fuelType + poly(mileage, 2):engineSize_f + tax:fuelType +
tax:engineSize_f + mpg:fuelType + mpg:transmission +
poly(price, 2):fuelType:engineSize_f + poly(mileage, 2):fuelType:engineSize_f +
mpg:fuelType:engineSize_f, family = "binomial", data = df_train[!df_train$mout ==
"MvOut.Yes",])
summary(m9)
Anova(m9, test="LR")
anova(m9, m8, test="Chisq")
AIC(m1, m2, m3, m4, m5, m6, m7 ,m8, m9)
dfwork <- df_train[!df_train$mout=="MvOut.Yes",]
Boxplot(abs(rstudent(m9)))
llres <- which(abs(rstudent(m9))>2.6);llres
Boxplot(hatvalues(m9))
influencePlot(m9, id=list(n=10))
Boxplot(cooks.distance(m9))
llout<-which(abs(cooks.distance(m9))>5000);llout
llrem<-unique(c(llout,llres));llrem
m10<- glm(formula = Audi ~ poly(price, 2) + poly(mileage, 2) + tax +
mpg + fuelType + transmission + engineSize_f + fuelType:transmission +
fuelType:engineSize_f + poly(price,
2):fuelType + poly(mileage,
2):fuelType + poly(mileage, 2):engineSize_f + tax:fuelType +
tax:engineSize_f + mpg:fuelType + mpg:transmission +
poly(price, 2):fuelType:engineSize_f + poly(mileage, 2):fuelType:engineSize_f +
mpg:fuelType:engineSize_f, family = "binomial", data=dfwork[-llrem,])
summary(m10)
AIC(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10)
plot(allEffects(m10), selection = 2)
plot(allEffects(m10), selection = 1)
plot(allEffects(m10), selection = 2)
plot(allEffects(m10), selection = 5)
marginalModelPlots(m10)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10))
outlierTest(m10)
influencePlot(m10)
# ROC Curve
library("pROC")
par(pty = "s")
roc(df_test$Audi, pred_test, plot = TRUE, legacy.axes = TRUE, percent = TRUE, col = "#377eb8", xlab="False Positive Percentage", ylab="True Positive Percentage", print.auc = TRUE)
pred_test_m9 <- predict(m9, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m9, percent = TRUE, col = "#4daf4a", print.auc = TRUE, add = TRUE, print.auc.y = 45)
pred_test_m8 <- predict(m8, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m8, percent = TRUE, col = "#ff0000", print.auc = TRUE, add = TRUE, print.auc.y = 40)
pred_test_m7 <- predict(m7, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m7, percent = TRUE, col = "#00EFFF", print.auc = TRUE, add = TRUE, print.auc.y = 35)
pred_test_m6 <- predict(m6, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m6, percent = TRUE, col = "#F700FF", print.auc = TRUE, add = TRUE, print.auc.y = 30)
pred_test_m5 <- predict(m5, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m5, percent = TRUE, col = "#FFCD00", print.auc = TRUE, add = TRUE, print.auc.y = 25)
pred_test_m4 <- predict(m4, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m4, percent = TRUE, col = "#00FF89", print.auc = TRUE, add = TRUE, print.auc.y = 20)
pred_test_m3 <- predict(m3, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m3, percent = TRUE, col = "#00BCFF", print.auc = TRUE, add = TRUE, print.auc.y = 15)
pred_test_m2 <- predict(m2, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m2, percent = TRUE, col = "#9A00FF", print.auc = TRUE, add = TRUE, print.auc.y = 10)
pred_test_m1 <- predict(m1, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m1, percent = TRUE, col = "#D5FF00", print.auc = TRUE, add = TRUE, print.auc.y = 5)
legend("bottomright", legend=c("m10","m9","m8","m7","m6","m5","m4","m3","m2","m1"), col=c("#377eb8","#4daf4a","#ff0000","#00EFFF","#F700FF","#FFCD00","#00FF89","#00BCFF","#9A00FF","#D5FF00"))
legend("bottomright", legend=c("m10","m9","m8","m7","m6","m5","m4","m3","m2","m1"), col=c("#377eb8","#4daf4a","#ff0000","#00EFFF","#F700FF","#FFCD00","#00FF89","#00BCFF","#9A00FF","#D5FF00"), lwd=4)
# ROC Curve
library("pROC")
par(pty = "s")
roc(df_test$Audi, pred_test, plot = TRUE, legacy.axes = TRUE, percent = TRUE, col = "#377eb8", xlab="False Positive Percentage", ylab="True Positive Percentage", print.auc = TRUE)
pred_test <- predict(m10, newdata=df_test, type="response")
ht <- hoslem.test(df_test$Audi, pred_test); ht
cbind(ht$observed, ht$expected)
# ROC Curve
library("pROC")
par(pty = "s")
roc(df_test$Audi, pred_test, plot = TRUE, legacy.axes = TRUE, percent = TRUE, col = "#377eb8", xlab="False Positive Percentage", ylab="True Positive Percentage", print.auc = TRUE)
pred_test_m9 <- predict(m9, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m9, percent = TRUE, col = "#4daf4a", print.auc = TRUE, add = TRUE, print.auc.y = 45)
pred_test_m8 <- predict(m8, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m8, percent = TRUE, col = "#ff0000", print.auc = TRUE, add = TRUE, print.auc.y = 40)
pred_test_m7 <- predict(m7, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m7, percent = TRUE, col = "#00EFFF", print.auc = TRUE, add = TRUE, print.auc.y = 35)
pred_test_m6 <- predict(m6, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m6, percent = TRUE, col = "#F700FF", print.auc = TRUE, add = TRUE, print.auc.y = 30)
pred_test_m5 <- predict(m5, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m5, percent = TRUE, col = "#FFCD00", print.auc = TRUE, add = TRUE, print.auc.y = 25)
pred_test_m4 <- predict(m4, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m4, percent = TRUE, col = "#00FF89", print.auc = TRUE, add = TRUE, print.auc.y = 20)
pred_test_m3 <- predict(m3, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m3, percent = TRUE, col = "#00BCFF", print.auc = TRUE, add = TRUE, print.auc.y = 15)
pred_test_m2 <- predict(m2, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m2, percent = TRUE, col = "#9A00FF", print.auc = TRUE, add = TRUE, print.auc.y = 10)
pred_test_m1 <- predict(m1, newdata=df_test, type="response")
plot.roc(df_test$Audi, pred_test_m1, percent = TRUE, col = "#D5FF00", print.auc = TRUE, add = TRUE, print.auc.y = 5)
legend("bottomright", legend=c("m10","m9","m8","m7","m6","m5","m4","m3","m2","m1"), col=c("#377eb8","#4daf4a","#ff0000","#00EFFF","#F700FF","#FFCD00","#00FF89","#00BCFF","#9A00FF","#D5FF00"))
legend("bottomright", legend=c("m10","m9","m8","m7","m6","m5","m4","m3","m2","m1"), col=c("#377eb8","#4daf4a","#ff0000","#00EFFF","#F700FF","#FFCD00","#00FF89","#00BCFF","#9A00FF","#D5FF00"), lwd=4)
# Confusion Table Analysis
treshold <- 0.5
audi.est <- ifelse(pred_test<treshold,0,1)
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)
prob.audi <- m3_1$fit
audi.est <- ifelse(prob.audi<0.5,0,1)
prob.audi <- m10$fit
audi.est <- ifelse(prob.audi<0.5,0,1)
tt<-table(audi.est,df_test$Audi);tt
audi.est <- ifelse(prob.audi<0.5,0,1)
tt<-table(audi.est,df_test$Audi);tt
library(knitr)
library(formatR)
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=70),tidy=TRUE)
setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable2")
#setwd("C:/Users/Arnau/Desktop/adei/deliverable3")
# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "corrplot", "moments")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
load("~/GitHub/adei/adei/deliverable3/5000cars_clean.RData")
hist(df$price,100,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$price);ss<-sd(df$price)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)
shapiro.test(df$price)
#moments library
skewness(df$price)
kurtosis(df$price)
ll<-which(df$years_after_sell==0)
df$years_after_sell[ll]<-0.5
ll<-which(df$tax==0)
df$tax[ll]<-0.5
ll<-which(df$mileage==0)
df$mileage[ll]<-0.5
ll<-which(df$mpg==0)
df$mpg[ll]<-0.5
ll<-which(df$mpg==0)
df$mpg[ll]<-0.5
## Start to find models
Steps to follow:
M = round(cor(df[,c("price",vars_con)], method="spearman"),dig=2)
corrplot(M,  method = 'circle', insig='blank',
addCoef.col ='black', number.cex = 0.8, order = 'AOE', diag=FALSE)
M = round(cor(df[,c("price",vars_con)], method="spearman"),dig=2);M
model_1 <- lm(price~mileage+tax+mpg+years_after_sell, data=df);summary(model_1)
library(MASS)
# Target variable transformation?
boxcox(price~mileage+tax+mpg+years_after_sell, data=df)
# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df)
summary(m2)
vif(m2) #Not changed because explanatory variables have not changed
# Transformations to my regresors
residualPlots(m2,id=list(method=cooks.distance(m2),n=10))
boxTidwell(log(price)~mileage+tax+years_after_sell, data=df)
boxTidwell(log(price)~mpg+years_after_sell+mileage, data=df)
# Suggested by BoxTidwell
m3<-lm(log(price)~I(mileage^3)+tax+I(mpg^-1)+I(years_after_sell^2),data=df)
m3_aux <- step(m3, k=log(nrow(df)) ) #to see if we can reduce the model
#step shows that no reduction is needed
summary(m3)
anova(m2, m3)
abs(AIC(m2, m3)) #model 3 offers a better fit
abs(BIC(m2, m3)) #model 3 offers a better fit
df_clean = df[!df$mout=="MvOut.Yes",]
m3<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
summary(m3)
boxTidwell(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
m3<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
summary(m3)
df_clean = df[!df$mout=="MvOut.Yes",]
m3<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
summary(m3)
boxTidwell(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
#we see from the boxtidwell output that the mileage does not need transformation and that the tax should be square rooted
boxTidwell(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
boxTidwell(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
#we see from the boxtidwell output that the mileage does not need transformation and that the tax should be square rooted
boxTidwell(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
m3_aux<-lm(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
summary(m3_aux)
m3 <- m3_aux
# Validation and effects consideration:
Anova(m3) #Net effect test
vif(m3)
par(mfrow=c(2,2))
plot(m3,id.n=0)
plot(m3)
par(mfrow=c(1,1))
library(lmtest)
bptest(m3)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10))
marginalModelPlots(m3)
avPlots(m3,id=list(method=hatvalues(m3),n=5))
crPlots(m3,id=list(method=cooks.distance(m3),n=5))
residualPlots(m3,id=list(method=cooks.distance(m3),n=10))
residualPlots(m3,id=list(method=cooks.distance(m3)))
library(effects)
plot(allEffects(m3))
df_clean$engineSize_num <- as.numeric(as.character(df_clean$engineSize))
hist(df_clean$engineSize_num)
s_aux <- summary(df_clean$engineSize_num)
df_clean$engineSize <- factor(cut(df_clean$engineSize_num,breaks=c(s_aux[1],2,4,s_aux[6]),include.lowest = T ), labels=c("small_engine", "medium_engine", "large_engine"))
barplot(table(df_clean$engineSize))
m4 <- update(m3, ~.+fuelType+transmission+engineSize+manufacturer,data=df_clean)
vif(m4)
#we do not have dependency among variables
summary(m4)
Anova(m4)
par(mfrow=c(2,2))
plot(m4,id.n=0)
par(mfrow=c(1,1))
#we do not have dependency among variables
summary(m4)
par(mfrow=c(2,2))
plot(m6,id.n=0)
m5<-update(m3, ~.+fuelType*(engineSize+transmission+manufacturer)^2, data=df_clean)
summary(m5)
#we will perform a step to remove the insignificant interactions from our model
m5_aux <- step(m5, k=log(nrow(df_clean)))
#from the step function we are left with this model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:engineSize+ fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
Anova(m5)
#we see from the Anova that the interaction between fuelType and engineSize has no relevancy, so we will remove it from our model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
kruskal.test(df_clean$tax~df_clean$manufacturer)
boxplot(df_clean$tax~df_clean$manufacturer) #not so strong correlation
m6 <- update(m5, ~.+tax:manufacturer, data=df_clean)
summary(m6)
#let's see if our model with improved interactions offers some difference from our previous model
anova(m6,m4) #it does
#let's compare the AIC of both models to see which has the better fit
abs(AIC(m6, m4)) #model 4 offers a better fit
abs(BIC(m6, m4)) #model 4 offers a better fit
#model 6 = 0.843
#model 4 = 0.8356
par(mfrow=c(2,2))
plot(m6,id.n=0)
par(mfrow=c(1,1))
library(olsrr)
hat.plot <-function(fit) {
p <- length(coefficients(fit))
n <- length(fitted(fit))
plot(hatvalues(fit), main="Index Plot of Hat Values")
abline(h=3*p/n, col="red", lty=2)
}
hat.plot(m6)
ols_plot_cooksd_chart(m6)
library(olsrr)
install.packages("olsrr")
library(olsrr)
ols_plot_cooksd_chart(m6)
# Define initial parameters to calculate obs. with high leverage:
p <-length(m6$coefficients)
n <- length(m6$fitted.values)
hat_param <- 3
# A priori influential observation
influencePlot(m6, id=list(n=10))
ll_priori_influential <- which(abs(hatvalues(m6))>hat_param*(p/n))
length(ll_priori_influential)
ll_rst <- which(abs(rstudent(m6))>3);
length(ll_rst)
ll_hat_rst <- unique(c(ll_priori_influential,ll_rst))
length(ll_hat_rst)
# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m6))>(4/(n-p)))
length(ll_posteriori_influential)
m7_hat_rst<-update(m6, data=df_clean[-ll_hat_rst,])
summary(m7_hat_rst)
m7_cook <- update(m6, data=df_clean[-ll_posteriori_influential,])
summary(m7_cook)
# Define initial parameters to calculate obs. with high leverage:
p <-length(m6$coefficients)
n <- length(m6$fitted.values)
hat_param <- 3
#we use as a threshold to lavel the leverage of the obs 3*p/n because we have a large dataset
# A priori influential observation
influencePlot(m6, id=list(n=10))
ll_priori_influential <- which(abs(hatvalues(m6))>hat_param*(p/n))
length(ll_priori_influential)
ll_rst <- which(abs(rstudent(m6))>3);
length(ll_rst)
ll_hat_rst <- unique(c(ll_priori_influential,ll_rst))
length(ll_hat_rst)
# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m6))>(4/(n-p)))
length(ll_posteriori_influential)
m7_hat_rst<-update(m6, data=df_clean[-ll_hat_rst,])
summary(m7_hat_rst)
m7_cook <- update(m6, data=df_clean[-ll_posteriori_influential,])
summary(m7_cook)
m7 <- m7_cook
Anova(m7)
#all variables and interactions are relevant
par(mfrow=c(1,1))
plot(allEffects(m7), selection=1)
plot(allEffects(m7), selection=2)
plot(m7,id.n=0)
par(mfrow=c(2,2))
plot(m7,id.n=0)
df_clean2=df_clean[-ll_posteriori_influential,]
res.pca <- PCA(df_clean2[c(5,7,8,13)], graph=FALSE )
plot(res.pca$ind$coord[,1], log(df_clean2$price), pch=19,col="darkblue", main="PCA of variables vs Actual/Predicted price")
points(res.pca$ind$coord[,1], m7$fitted.values, col="red", pch=20)
legend("topright", legend=c("Actual", "Predicted"),
col=c("darkblue", "red"), pch=19, cex=0.8,
title="Point types")
