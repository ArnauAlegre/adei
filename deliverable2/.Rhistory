# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr", "lmtest","effects", "statmod", "DescTools","ResourceSelection","chemometrics","missMDA")
#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
#verify they are loaded
search()
# Clear plots
if(!is.null(dev.list())) dev.off()
# Clean workspace
rm(list=ls())
setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable3")
filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable3/"
load(paste0(filepath,"5000cars_clean.RData"))
# 80% train sample and 20% test sample
llwork <- sample(1:nrow(df),round(0.80*nrow(df),0))
df_train <- df[llwork,]
df_test <-df[-llwork,]
ll<-which(df_train$tax==0)
df_train$tax[ll]<-0.5
m1<-glm(Audi~price+mileage+tax+mpg+years_after_sell,family="binomial",data=df_train)
summary(m1)
vif(m1)
m2 <- glm(Audi~log(price)+mileage+tax+poly(mpg,2)+poly(years_after_sell,2),family="binomial",data=df_train)
summary(m2)
m3 <- glm(Audi~log(price)+mileage+tax+poly(mpg,2)+poly(years_after_sell,2),family="binomial",data=df_train[!df_train$mout=="MvOut.Yes",])
summary(m3)
m4 <- step(m3) #esto no realiza nada
x2 <- cbind(df_train$mpg[!df_train$mout=="YesMOut"])
z<-glm.scoretest(m4,x2);z # library statmod
2*(1-pnorm(abs(z)))
llres <- which(abs(rstudent(m3))>2.5);length(llres)
df_train[llres,]
df_train$engineSize_f <-as.numeric(levels(df_train$engineSize))[df_train$engineSize]
par(mfrow=c(1,1))
hist(df_train$engineSize_f)
quantile(df_train$engineSize_f, c(0.3333333,0.6666666,1))
df_train$engineSize_f <- factor(cut(df_train$engineSize_f, breaks = c(0,1.6,2,10)))
df_test$engineSize_f <- as.numeric(levels(df_test$engineSize))[df_test$engineSize]
df_test$engineSize_f <- factor(cut(df_test$engineSize_f, breaks =  c(0,1.6,2,10)))
table(df_train$engineSize_f)
table(df_test$engineSize_f)
m5 <- update(m4,~.+fuelType+transmission+engineSize_f,data=df_train[!df_train$mout=="YesMOut",])
vif(m5)
summary(m5)
Anova(m5, test="LR")
crPlots(m5,id=list(method=cooks.distance(m5),n=5))
# library(effects)
plot(allEffects(m5), selection = 1) #ni idea d q hace
m6<- step(m5)
plot(allEffects(m6))
AIC(m1,m2,m3,m4,m5,m6)
summary(m6)
m7 <- update(m6, ~.*(fuelType+transmission+engineSize_f)^2,data=df_train[!df_train$mout=="YesMOut",])
vif(m7)
summary(m7)
m8<- step(m7)
vif(m8)
summary(m8)
# H0: Model fits data
Anova(m8)
X2m8<-sum((resid(m8,"pearson")^2));X2m8
1-pchisq(X2m8, m8$df.res)
# PseudoR2
library(DescTools)
PseudoR2(m8, which='all') # Not working for grouped data
# Sheather
1 - (m8$deviance / m8$null.deviance)
# McFadden
1-(as.numeric(logLik(m8))/as.numeric(logLik(8)))
library(ResourceSelection)
pred_test <- predict(m8, newdata=df_test, type="response")1
pred_test <- predict(m8, newdata=df_test, type="response")
ht <- hoslem.test(df_test$Audi, pred_test)
ht
cbind(ht$observed, ht$expected)
library("ROCR")
library("AUC")
dadesroc<-prediction(pred_test,df_test$Audi)
par(mfrow=c(1,2))
par(mfrow=c(1,2))
performance(dadesroc,"auc",fpr.stop=0.05)
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)
roc(pred_test,df_test$Audi)
abline(0,1,lty=2)
library(cvAUC)
AUC(pred_test,df_test$Audi)
# Confusion Table Analysis
audi.est <- ifelse(pred_test<0.5,0,1)
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)
# Model na?ve
prob.audi <- m0$fit
# Confusion Table Analysis
audi.est <- ifelse(pred_test<0.5,0,1)
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)
# Model na?ve
prob.audi <- m0$fit
# Model na?ve
prob.audi <- m1$fit
audi.est <- ifelse(prob.audi<0.5,0,1)
tt<-table(audi.est,dfwork$Audi[-llrem]);tt
setwd("C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable2")
#setwd("C:/Users/Arnau/Desktop/adei/deliverable3")
# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "corrplot", "moments")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/adei/"
#filepath <- "C:/Users/Arnau/Desktop/adei/deliverable3/"
load(paste0(filepath,"5000cars_clean.RData"))
# dim(df)       # Displays the sample size
# names(df)     # Displays the names of the sample variables
# summary(df)
#filepath <- "C:/Users/Arnau/Desktop/adei/deliverable3/"
load(paste0(filepath,"5000cars_clean.RData"))
filepath<-"C:/Users/TOREROS-II/Documents/GitHub/adei/adei/deliverable3/"
#filepath <- "C:/Users/Arnau/Desktop/adei/deliverable3/"
load(paste0(filepath,"5000cars_clean.RData"))
hist(df$price,100,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$price);ss<-sd(df$price)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)
shapiro.test(df$price)
#moments library
skewness(df$price)
kurtosis(df$price)
ll<-which(df$years_after_sell==0)
df$years_after_sell[ll]<-0.5
ll<-which(df$tax==0)
df$tax[ll]<-0.5
ll<-which(df$mileage==0)
df$mileage[ll]<-0.5
ll<-which(df$mpg==0)
df$mpg[ll]<-0.5
M = round(cor(df[,c("price",vars_con)], method="spearman"),dig=2)
corrplot(M,  method = 'circle', insig='blank',
addCoef.col ='black', number.cex = 0.8, order = 'AOE', diag=FALSE)
model_1 <- lm(price~mileage+tax+mpg+years_after_sell, data=df);summary(model_1)
library(MASS)
# Target variable transformation?
boxcox(price~mileage+tax+mpg+years_after_sell, data=df)
# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df)
summary(m2)
vif(m2) #Not changed because explanatory variables have not changed
# Transformations to my regresors
residualPlots(m2,id=list(method=cooks.distance(m2),n=10))
boxTidwell(log(price)~mileage+tax+years_after_sell, data=df)
boxTidwell(log(price)~mpg+years_after_sell+mileage, data=df)
# Suggested by BoxTidwell
m3<-lm(log(price)~I(mileage^3)+tax+I(mpg^-1)+I(years_after_sell^2),data=df)
m3_aux <- step(m3, k=log(nrow(df)) ) #to see if we can reduce the model
#step shows that no reduction is needed
summary(m3)
anova(m2, m3)
abs(AIC(m2, m3)) #model 2 offers a better fit
df_clean = df[!df$mout=="MvOut.Yes",]
m3<-lm(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
summary(m3)
boxTidwell(log(price)~mileage+tax+mpg+years_after_sell,data=df_clean)
#we see from the boxtidwell output that the mileage does not need transformation and that the tax should be square rooted
boxTidwell(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
m3_aux<-lm(log(price)~mileage+sqrt(tax)+I(mpg^-2)+I(years_after_sell^2),data=df_clean)
summary(m3_aux)
m3 <- m3_aux
# Validation and effects consideration:
Anova(m3) #Net effect test
par(mfrow=c(2,2))
plot(m3,id.n=0)
par(mfrow=c(1,1))
library(lmtest)
bptest(m3)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10))
marginalModelPlots(m3)
avPlots(m3,id=list(method=hatvalues(m3),n=5))
df_clean$engineSize_num <- as.numeric(as.character(df_clean$engineSize))
hist(df_clean$engineSize_num)
s_aux <- summary(df_clean$engineSize_num)
df_clean$engineSize <- factor(cut(df_clean$engineSize_num,breaks=c(s_aux[1],2,4,s_aux[6]),include.lowest = T ), labels=c("small_engine", "medium_engine", "large_engine"))
barplot(table(df_clean$engineSize))
m4 <- update(m3, ~.+fuelType+transmission+engineSize+manufacturer,data=df_clean)
vif(m4)
#we do not have dependency among variables
summary(m4)
Anova(m4)
#we see from the Anova output that all variables greatly contribute to the regression model
par(mfrow=c(2,2))
plot(m4,id.n=0)
par(mfrow=c(1,1))
m5<-update(m3, ~.+fuelType*(engineSize+transmission+manufacturer)^2, data=df_clean)
summary(m5)
#we will perform a step to remove the insignificant interactions from our model
m5_aux <- step( m5, k=log(nrow(df_clean)))
#from the step function we are left with this model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:engineSize+ fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
Anova(m5)
#we see from the Anova that the interaction between fuelType and engineSize has no relevancy, so we will remove it from our model
m5 <- lm(log(price) ~ mileage + sqrt(tax) + I(mpg^-2) + I(years_after_sell^2) + fuelType + engineSize + transmission + manufacturer + engineSize:manufacturer + fuelType:manufacturer + fuelType:engineSize:manufacturer, data=df_clean)
kruskal.test(df_clean$tax~df_clean$manufacturer)
boxplot(df_clean$tax~df_clean$manufacturer) #not so strong correlation
m6 <- update(m5, ~.+tax:manufacturer, data=df_clean)
summary(m6)
#let's see if our model with improved interactions offers some difference from our previous model
anova(m6,m4) #it does
#let's compare the AIC of both models to see which has the better fit
abs(AIC(m6, m4)) #model 4 offers a better fit
abs(BIC(m6, m4)) #model 4 offers a better fit
par(mfrow=c(2,2))
plot(m6,id.n=0)
par(mfrow=c(1,1))
library(olsrr)
hat.plot <-function(fit) {
p <- length(coefficients(fit))
n <- length(fitted(fit))
plot(hatvalues(fit), main="Index Plot of Hat Values")
abline(h=3*p/n, col="red", lty=2)
}
hat.plot(m6)
Anova(m7)
# Define initial parameters to calculate obs. with high leverage:
p <-length(m6$coefficients)
n <- length(m6$fitted.values)
hat_param <- 3
#we use as a threshold to lavel the leverage of the obs 3*p/n because we have a large dataset
# A priori influential observation
influencePlot(m6, id=list(n=10))
ll_priori_influential <- which(abs(hatvalues(m6))>hat_param*(p/n))
length(ll_priori_influential)
ll_rst <- which(abs(rstudent(m6))>3);
length(ll_rst)
ll_hat_rst <- unique(c(ll_priori_influential,ll_rst))
length(ll_hat_rst)
# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m6))>(4/(n-p)))
length(ll_posteriori_influential)
m7_hat_rst<-update(m6, data=df_clean[-ll_hat_rst,])
summary(m7_hat_rst)
m7_cook <- update(m6, data=df_clean[-ll_posteriori_influential,])
summary(m7_cook)
m7 <- m7_cook
